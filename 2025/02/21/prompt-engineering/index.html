<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Always learning and exploring the frontiers of AI."><link href=https://dannyzheng.me/2025/02/21/prompt-engineering/ rel=canonical><link href=../../../01/12/building-effective-agents/ rel=prev><link href=../../../04/15/why-reinforcement-learning-rl-is-hot-again/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../feed_rss_updated.xml><link rel=icon href=../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>Prompt Engineering - Danny's Share</title><link rel=stylesheet href=../../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1%207.775V2.75C1%201.784%201.784%201%202.75%201h5.025c.464%200%20.91.184%201.238.513l6.25%206.25a1.75%201.75%200%200%201%200%202.474l-5.026%205.026a1.75%201.75%200%200%201-2.474%200l-6.25-6.25A1.75%201.75%200%200%201%201%207.775m1.5%200c0%20.066.026.13.073.177l6.25%206.25a.25.25%200%200%200%20.354%200l5.025-5.025a.25.25%200%200%200%200-.354l-6.25-6.25a.25.25%200%200%200-.177-.073H2.75a.25.25%200%200%200-.25.25ZM6%205a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.5%201.75v11.5c0%20.138.112.25.25.25h3.17a.75.75%200%200%201%200%201.5H2.75A1.75%201.75%200%200%201%201%2013.25V1.75C1%20.784%201.784%200%202.75%200h8.5C12.216%200%2013%20.784%2013%201.75v7.736a.75.75%200%200%201-1.5%200V1.75a.25.25%200%200%200-.25-.25h-8.5a.25.25%200%200%200-.25.25m13.274%209.537zl-4.557%204.45a.75.75%200%200%201-1.055-.008l-1.943-1.95a.75.75%200%200%201%201.062-1.058l1.419%201.425%204.026-3.932a.75.75%200%201%201%201.048%201.074M4.75%204h4.5a.75.75%200%200%201%200%201.5h-4.5a.75.75%200%200%201%200-1.5M4%207.75A.75.75%200%200%201%204.75%207h2a.75.75%200%200%201%200%201.5h-2A.75.75%200%200%201%204%207.75%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M3.499.75a.75.75%200%200%201%201.5%200v.996C5.9%202.903%206.793%203.65%207.662%204.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873%2010.794-.045%2012.622.26%2014.408.558%2016%201.94%2016%204.25c0%201.278-.954%202.575-2.44%202.734l.146.508.065.22c.203.701.412%201.455.476%202.226.142%201.707-.4%203.03-1.487%203.898C11.714%2014.671%2010.27%2015%208.75%2015h-6a.75.75%200%200%201%200-1.5h1.376a4.5%204.5%200%200%201-.563-1.191%203.84%203.84%200%200%201-.05-2.063%204.65%204.65%200%200%201-2.025-.293.75.75%200%200%201%20.525-1.406c1.357.507%202.376-.006%202.698-.318l.009-.01a.747.747%200%200%201%201.06%200%20.75.75%200%200%201-.012%201.074c-.912.92-.992%201.835-.768%202.586.221.74.745%201.337%201.196%201.621H8.75c1.343%200%202.398-.296%203.074-.836.635-.507%201.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4%202.4%200%200%201-.507-.441%203.1%203.1%200%200%201-.633-1.248.75.75%200%200%201%201.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738%200%201.25-.615%201.25-1.25%200-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706%201.345-.46.92-.27%201.774.019%203.062l.042.19.01.05c.348.443.666.949.94%201.553a.75.75%200%201%201-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7%205.527c-.814-.68-1.75-1.462-2.692-2.619a3.7%203.7%200%200%200-1.023.88c-.406.495-.663%201.036-.722%201.508.116.122.306.21.591.239.388.038.797-.06%201.032-.19a.75.75%200%200%201%20.728%201.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75%205.677V5.5c0-.984.48-1.94%201.077-2.664.46-.559%201.05-1.055%201.673-1.353z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M13.78%204.22a.75.75%200%200%201%200%201.06l-7.25%207.25a.75.75%200%200%201-1.06%200L2.22%209.28a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018L6%2010.94l6.72-6.72a.75.75%200%200%201%201.06%200%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.92%206.085h.001a.749.749%200%201%201-1.342-.67c.169-.339.436-.701.849-.977C6.845%204.16%207.369%204%208%204a2.76%202.76%200%200%201%201.637.525c.503.377.863.965.863%201.725%200%20.448-.115.83-.329%201.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6%206%200%200%200-.26.16%201%201%200%200%200-.276.245.75.75%200%200%201-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1%201%200%200%200%20.277-.245C8.96%206.514%209%206.427%209%206.25a.61.61%200%200%200-.262-.525A1.27%201.27%200%200%200%208%205.5c-.369%200-.595.09-.74.187a1%201%200%200%200-.34.398M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M6.457%201.047c.659-1.234%202.427-1.234%203.086%200l6.082%2011.378A1.75%201.75%200%200%201%2014.082%2015H1.918a1.75%201.75%200%200%201-1.543-2.575Zm1.763.707a.25.25%200%200%200-.44%200L1.698%2013.132a.25.25%200%200%200%20.22.368h12.164a.25.25%200%200%200%20.22-.368Zm.53%203.996v2.5a.75.75%200%200%201-1.5%200v-2.5a.75.75%200%200%201%201.5%200M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.344%202.343za8%208%200%200%201%2011.314%2011.314A8.002%208.002%200%200%201%20.234%2010.089a8%208%200%200%201%202.11-7.746m1.06%2010.253a6.5%206.5%200%201%200%209.108-9.275%206.5%206.5%200%200%200-9.108%209.275M6.03%204.97%208%206.94l1.97-1.97a.749.749%200%200%201%201.275.326.75.75%200%200%201-.215.734L9.06%208l1.97%201.97a.749.749%200%200%201-.326%201.275.75.75%200%200%201-.734-.215L8%209.06l-1.97%201.97a.749.749%200%200%201-1.275-.326.75.75%200%200%201%20.215-.734L6.94%208%204.97%206.03a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M9.504.43a1.516%201.516%200%200%201%202.437%201.713L10.415%205.5h2.123c1.57%200%202.346%201.909%201.22%203.004l-7.34%207.142a1.25%201.25%200%200%201-.871.354h-.302a1.25%201.25%200%200%201-1.157-1.723L5.633%2010.5H3.462c-1.57%200-2.346-1.909-1.22-3.004zm1.047%201.074L3.286%208.571A.25.25%200%200%200%203.462%209H6.75a.75.75%200%200%201%20.694%201.034l-1.713%204.188%206.982-6.793A.25.25%200%200%200%2012.538%207H9.25a.75.75%200%200%201-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005%200-.009.004%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M4.72.22a.75.75%200%200%201%201.06%200l1%20.999a3.5%203.5%200%200%201%202.441%200l.999-1a.748.748%200%200%201%201.265.332.75.75%200%200%201-.205.729l-.775.776c.616.63.995%201.493.995%202.444v.327q0%20.15-.025.292c.408.14.764.392%201.029.722l1.968-.787a.75.75%200%200%201%20.556%201.392L13%207.258V9h2.25a.75.75%200%200%201%200%201.5H13v.5q-.002.615-.141%201.186l2.17.868a.75.75%200%200%201-.557%201.392l-2.184-.873A5%205%200%200%201%208%2016a5%205%200%200%201-4.288-2.427l-2.183.873a.75.75%200%200%201-.558-1.392l2.17-.868A5%205%200%200%201%203%2011v-.5H.75a.75.75%200%200%201%200-1.5H3V7.258L.971%206.446a.75.75%200%200%201%20.558-1.392l1.967.787c.265-.33.62-.583%201.03-.722a1.7%201.7%200%200%201-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72%201.28a.75.75%200%200%201%200-1.06m.53%206.28a.75.75%200%200%200-.75.75V11a3.5%203.5%200%201%200%207%200V7.25a.75.75%200%200%200-.75-.75ZM6.173%205h3.654A.17.17%200%200%200%2010%204.827V4.5a2%202%200%201%200-4%200v.327c0%20.096.077.173.173.173%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5%205.782V2.5h-.25a.75.75%200%200%201%200-1.5h6.5a.75.75%200%200%201%200%201.5H11v3.282l3.666%205.76C15.619%2013.04%2014.543%2015%2012.767%2015H3.233c-1.776%200-2.852-1.96-1.899-3.458Zm-2.4%206.565a.75.75%200%200%200%20.633%201.153h9.534a.75.75%200%200%200%20.633-1.153L12.225%2010.5h-8.45ZM9.5%202.5h-3V6c0%20.143-.04.283-.117.403L4.73%209h6.54L9.617%206.403A.75.75%200%200%201%209.5%206Z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1.75%202.5h10.5a.75.75%200%200%201%200%201.5H1.75a.75.75%200%200%201%200-1.5m4%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5m0%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5M2.5%207.75v6a.75.75%200%200%201-1.5%200v-6a.75.75%200%200%201%201.5%200%22/%3E%3C/svg%3E');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="Prompt Engineering - Danny's Share"><meta property=og:description content="Always learning and exploring the frontiers of AI."><meta property=og:image content=https://dannyzheng.me/assets/images/social/posts/my_blogs/prompt-engineering.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://dannyzheng.me/2025/02/21/prompt-engineering/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Prompt Engineering - Danny's Share"><meta name=twitter:description content="Always learning and exploring the frontiers of AI."><meta name=twitter:image content=https://dannyzheng.me/assets/images/social/posts/my_blogs/prompt-engineering.png></head> <body dir=ltr data-md-color-scheme=youtube data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#content class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Danny's Share" class="md-header__button md-logo" aria-label="Danny's Share" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Danny's Share </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Prompt Engineering </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=youtube data-md-color-primary=indigo data-md-color-accent=indigo aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../posts/podcasts/ class=md-tabs__link> Podcasts </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Danny's Share" class="md-nav__button md-logo" aria-label="Danny's Share" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Danny's Share </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../../.. class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=true> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/ai-agents/ class=md-nav__link> <span class=md-ellipsis> AI Agents </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/ai-engineering/ class=md-nav__link> <span class=md-ellipsis> AI Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/growth/ class=md-nav__link> <span class=md-ellipsis> Growth </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/model-context-protocol/ class=md-nav__link> <span class=md-ellipsis> Model Context Protocol </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/productivity/ class=md-nav__link> <span class=md-ellipsis> Productivity </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/prompt-engineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/retrieval-augmented-generation/ class=md-nav__link> <span class=md-ellipsis> Retrieval-Augmented Generation </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/startup/ class=md-nav__link> <span class=md-ellipsis> Startup </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../posts/podcasts/ class=md-nav__link> <span class=md-ellipsis> Podcasts </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#content class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=#what-is-prompt-engineering class=md-nav__link> <span class=md-ellipsis> What is prompt engineering? </span> </a> </li> <li class=md-nav__item> <a href=#why-prompt-engineering-matters class=md-nav__link> <span class=md-ellipsis> Why prompt engineering matters? </span> </a> <nav class=md-nav aria-label="Why prompt engineering matters?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-medprompt-a-prompting-technique-from-a-study-by-microsoft-enhances-the-performance-of-gpt-4 class=md-nav__link> <span class=md-ellipsis> 1. Medprompt (a prompting technique from a study by Microsoft) enhances the performance of GPT-4: </span> </a> <nav class=md-nav aria-label="1. Medprompt (a prompting technique from a study by Microsoft) enhances the performance of GPT-4:"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#medprompt-allows-gpt-4-to-compete-with-fine-tuned-models-gpt-4-vs-fine-tuning class=md-nav__link> <span class=md-ellipsis> MedPrompt allows GPT-4 to compete with fine-tuned models. (GPT-4 vs Fine-tuning) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-boosting-lower-tier-models-with-effective-prompting-gpt-35-vs-gpt-4 class=md-nav__link> <span class=md-ellipsis> 2. Boosting Lower-Tier Models with Effective Prompting. (GPT-3.5 vs GPT-4) </span> </a> <nav class=md-nav aria-label="2. Boosting Lower-Tier Models with Effective Prompting. (GPT-3.5 vs GPT-4)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#you-may-ask-why-not-always-use-the-most-advanced-models class=md-nav__link> <span class=md-ellipsis> You may ask, why not always use the most advanced models? </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#basics-of-prompt-engineering class=md-nav__link> <span class=md-ellipsis> Basics of prompt engineering </span> </a> </li> <li class=md-nav__item> <a href=#the-prompt-engineering-lifecycle class=md-nav__link> <span class=md-ellipsis> The prompt engineering lifecycle </span> </a> </li> <li class=md-nav__item> <a href=#inference-parameters class=md-nav__link> <span class=md-ellipsis> Inference Parameters </span> </a> </li> <li class=md-nav__item> <a href=#lets-look-at-some-of-the-most-commonly-used-prompting-techniques class=md-nav__link> <span class=md-ellipsis> Let's look at some of the most commonly used prompting techniques: </span> </a> <nav class=md-nav aria-label="Let's look at some of the most commonly used prompting techniques:"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#zero-shot class=md-nav__link> <span class=md-ellipsis> Zero-Shot </span> </a> </li> <li class=md-nav__item> <a href=#few-shot class=md-nav__link> <span class=md-ellipsis> Few-Shot </span> </a> <nav class=md-nav aria-label=Few-Shot> <ul class=md-nav__list> <li class=md-nav__item> <a href=#for-maximum-effectiveness-make-sure-that-your-examples-are class=md-nav__link> <span class=md-ellipsis> For maximum effectiveness, make sure that your examples are: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#chain-of-thought class=md-nav__link> <span class=md-ellipsis> Chain of thought </span> </a> <nav class=md-nav aria-label="Chain of thought"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-let-llm-think class=md-nav__link> <span class=md-ellipsis> Why let LLM think? </span> </a> </li> <li class=md-nav__item> <a href=#why-not-let-llm-think class=md-nav__link> <span class=md-ellipsis> Why not let LLM think? </span> </a> </li> <li class=md-nav__item> <a href=#how-to-prompt-for-thinking class=md-nav__link> <span class=md-ellipsis> How to prompt for thinking </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#self-consistency class=md-nav__link> <span class=md-ellipsis> Self-Consistency </span> </a> </li> <li class=md-nav__item> <a href=#more-prompting-techiques-if-you-are-interested class=md-nav__link> <span class=md-ellipsis> More prompting techiques if you are interested. </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#general-tips-for-designing-prompts class=md-nav__link> <span class=md-ellipsis> General Tips for Designing Prompts </span> </a> <nav class=md-nav aria-label="General Tips for Designing Prompts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-start-simple-source class=md-nav__link> <span class=md-ellipsis> 1. Start Simple: (source) </span> </a> </li> <li class=md-nav__item> <a href=#2-be-clear-direct-and-detailed-source class=md-nav__link> <span class=md-ellipsis> 2. Be clear, direct, and detailed: (source) </span> </a> <nav class=md-nav aria-label="2. Be clear, direct, and detailed: (source)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#when-in-doubt-follow-the-golden-rule-of-clear-prompting class=md-nav__link> <span class=md-ellipsis> When in doubt, follow the Golden Rule of Clear Prompting: </span> </a> </li> <li class=md-nav__item> <a href=#how-to-be-clear-contextual-and-specific class=md-nav__link> <span class=md-ellipsis> How to be clear, contextual, and specific: </span> </a> </li> <li class=md-nav__item> <a href=#unclear-prompt class=md-nav__link> <span class=md-ellipsis> Unclear Prompt </span> </a> </li> <li class=md-nav__item> <a href=#clear-prompt class=md-nav__link> <span class=md-ellipsis> Clear Prompt </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-giving-llm-a-role-with-a-system-prompt-source class=md-nav__link> <span class=md-ellipsis> 3. Giving LLM a role with a system prompt: (source) </span> </a> <nav class=md-nav aria-label="3. Giving LLM a role with a system prompt: (source)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#financial-analysis-without-role-prompting class=md-nav__link> <span class=md-ellipsis> Financial analysis without role prompting. </span> </a> </li> <li class=md-nav__item> <a href=#financial-analysis-with-role-prompting class=md-nav__link> <span class=md-ellipsis> Financial analysis with role prompting. </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-put-instructions-at-the-beginning-of-the-prompt-and-use-delimiters-like-or-or-xml-tags-to-separate-the-instruction-and-context-source class=md-nav__link> <span class=md-ellipsis> 4. Put instructions at the beginning of the prompt and use delimiters like, ### or """, or XML tags to separate the instruction and context. (source) </span> </a> </li> <li class=md-nav__item> <a href=#5-be-specific-descriptive-and-as-detailed-as-possible-about-the-desired-context-outcome-length-format-style-etc-source class=md-nav__link> <span class=md-ellipsis> 5. Be specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc. (source) </span> </a> </li> <li class=md-nav__item> <a href=#6-reduce-fluffy-and-imprecise-descriptions-source class=md-nav__link> <span class=md-ellipsis> 6. Reduce ‚Äúfluffy‚Äù and imprecise descriptions: (source) </span> </a> </li> <li class=md-nav__item> <a href=#7-instead-of-just-saying-what-not-to-do-say-what-to-do-instead-source class=md-nav__link> <span class=md-ellipsis> 7. Instead of just saying what not to do, say what to do instead: (source) </span> </a> </li> <li class=md-nav__item> <a href=#8-code-generation-specific-use-leading-words-to-nudge-the-model-toward-a-particular-pattern-source class=md-nav__link> <span class=md-ellipsis> 8. Code Generation Specific - Use ‚Äúleading words‚Äù to nudge the model toward a particular pattern: (source) </span> </a> </li> <li class=md-nav__item> <a href=#9-use-prompt-templates class=md-nav__link> <span class=md-ellipsis> 9. Use prompt templates </span> </a> </li> <li class=md-nav__item> <a href=#10-long-context-prompting-tips class=md-nav__link> <span class=md-ellipsis> 10. Long context prompting tips </span> </a> </li> <li class=md-nav__item> <a href=#11-chain-complex-prompts class=md-nav__link> <span class=md-ellipsis> 11. Chain complex prompts </span> </a> <nav class=md-nav aria-label="11. Chain complex prompts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-chained-workflows class=md-nav__link> <span class=md-ellipsis> Example chained workflows: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#12-tool-use-function-calling class=md-nav__link> <span class=md-ellipsis> 12. Tool use (function calling) </span> </a> </li> <li class=md-nav__item> <a href=#improving-tool-use-quality-and-reliability class=md-nav__link> <span class=md-ellipsis> Improving tool use quality and reliability </span> </a> <nav class=md-nav aria-label="Improving tool use quality and reliability"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#a-provide-more-details-in-your-function-definition class=md-nav__link> <span class=md-ellipsis> a. Provide more details in your function definition </span> </a> </li> <li class=md-nav__item> <a href=#b-provide-more-context-in-the-system-message class=md-nav__link> <span class=md-ellipsis> b. Provide more context in the system message </span> </a> </li> <li class=md-nav__item> <a href=#c-instruct-the-model-to-ask-clarifying-questions class=md-nav__link> <span class=md-ellipsis> c. Instruct the model to ask clarifying questions </span> </a> </li> <li class=md-nav__item> <a href=#d-offload-the-burden-from-the-model-and-use-code-where-possible class=md-nav__link> <span class=md-ellipsis> d. Offload the burden from the model and use code where possible. </span> </a> </li> <li class=md-nav__item> <a href=#e-keep-the-number-of-functions-small-for-higher-accuracy class=md-nav__link> <span class=md-ellipsis> e. Keep the number of functions small for higher accuracy. </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#13-increse-output-consistency-json-modestructured-output class=md-nav__link> <span class=md-ellipsis> 13. Increse output consistency (JSON mode/Structured Output) </span> </a> <nav class=md-nav aria-label="13. Increse output consistency (JSON mode/Structured Output)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#a-four-effective-ways-to-generate-structured-outputs class=md-nav__link> <span class=md-ellipsis> a. Four Effective Ways to Generate Structured Outputs </span> </a> </li> <li class=md-nav__item> <a href=#b-when-to-use-structured-outputs-via-function-calling-vs-via-response_format class=md-nav__link> <span class=md-ellipsis> b. When to use Structured Outputs via function calling vs via response_format </span> </a> </li> <li class=md-nav__item> <a href=#c-structured-outputs-vs-json-mode class=md-nav__link> <span class=md-ellipsis> c. Structured Outputs vs JSON mode </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#14-reducing-latency class=md-nav__link> <span class=md-ellipsis> 14. Reducing Latency </span> </a> <nav class=md-nav aria-label="14. Reducing Latency"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-to-reduce-latency class=md-nav__link> <span class=md-ellipsis> How to reduce latency: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#15-avoiding-hallucinations class=md-nav__link> <span class=md-ellipsis> 15. Avoiding hallucinations </span> </a> <nav class=md-nav aria-label="15. Avoiding hallucinations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#a-allow-llm-to-say-i-dont-know class=md-nav__link> <span class=md-ellipsis> a. Allow LLM to say ‚ÄúI don‚Äôt know‚Äù. </span> </a> </li> <li class=md-nav__item> <a href=#b-use-direct-quotes-for-factual-grounding class=md-nav__link> <span class=md-ellipsis> b. Use direct quotes for factual grounding: </span> </a> </li> <li class=md-nav__item> <a href=#c-verify-with-citations class=md-nav__link> <span class=md-ellipsis> c. Verify with citations: </span> </a> </li> <li class=md-nav__item> <a href=#d-advanced-techniques class=md-nav__link> <span class=md-ellipsis> d. Advanced techniques: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#16-split-complex-tasks-into-simpler-subtasks class=md-nav__link> <span class=md-ellipsis> 16. Split complex tasks into simpler subtasks </span> </a> </li> <li class=md-nav__item> <a href=#17-test-changes-systematically class=md-nav__link> <span class=md-ellipsis> 17. Test changes systematically </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#common-misconceptions-about-prompts class=md-nav__link> <span class=md-ellipsis> Common Misconceptions About Prompts </span> </a> <nav class=md-nav aria-label="Common Misconceptions About Prompts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-a-prompt-is-static-write-it-once-and-youre-done class=md-nav__link> <span class=md-ellipsis> 1. A prompt is static; write it once and you‚Äôre done. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#2-prompts-require-perfect-grammar-and-punctuation class=md-nav__link> <span class=md-ellipsis> 2. Prompts require perfect grammar and punctuation. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#3-you-have-to-trick-the-model-into-working class=md-nav__link> <span class=md-ellipsis> 3. You have to ‚Äòtrick‚Äô the model into working. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#4-prompt-engineering-is-all-about-crafting-a-perfect-instruction class=md-nav__link> <span class=md-ellipsis> 4. Prompt engineering is all about crafting a perfect instruction. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#5-prompt-engineering-is-purely-about-writing-skill class=md-nav__link> <span class=md-ellipsis> 5. Prompt engineering is purely about writing skill. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#6-more-examples-always-produce-better-prompts class=md-nav__link> <span class=md-ellipsis> 6. More examples always produce better prompts. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#7-you-should-avoid-giving-the-model-too-much-information class=md-nav__link> <span class=md-ellipsis> 7. You should avoid giving the model too much information. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#8-role-playing-prompts-always-work class=md-nav__link> <span class=md-ellipsis> 8. Role-playing prompts always work. ‚ùå </span> </a> </li> <li class=md-nav__item> <a href=#9-once-you-find-a-good-prompt-itll-work-forever class=md-nav__link> <span class=md-ellipsis> 9. Once you find a good prompt, it‚Äôll work forever. ‚ùå </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#prompt-generator-tools class=md-nav__link> <span class=md-ellipsis> Prompt Generator Tools </span> </a> <nav class=md-nav aria-label="Prompt Generator Tools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-anthropic-prompt-generator class=md-nav__link> <span class=md-ellipsis> 1. Anthropic Prompt Generator </span> </a> </li> <li class=md-nav__item> <a href=#2-azure-ai-foundry class=md-nav__link> <span class=md-ellipsis> 2. Azure AI Foundry </span> </a> </li> <li class=md-nav__item> <a href=#3-microsoft-365-copilot class=md-nav__link> <span class=md-ellipsis> 3. Microsoft 365 Copilot </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#reasoning-model-ie-openai-o1deepseek-r1 class=md-nav__link> <span class=md-ellipsis> Reasoning Model (i.e. OpenAI o1/DeepSeek R1) </span> </a> <nav class=md-nav aria-label="Reasoning Model (i.e. OpenAI o1/DeepSeek R1)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#a-how-reasoning-works class=md-nav__link> <span class=md-ellipsis> a. How reasoning works </span> </a> </li> <li class=md-nav__item> <a href=#b-advice-on-reasoning-model-prompting class=md-nav__link> <span class=md-ellipsis> b. Advice on reasoning model prompting </span> </a> </li> <li class=md-nav__item> <a href=#c-below-example-illustrates-the-difference-between-the-non-reasoning-model-and-the-reasoning-model class=md-nav__link> <span class=md-ellipsis> c. Below example illustrates the difference between the non-reasoning model and the reasoning model: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src="https://avatars.githubusercontent.com/u/16627641?v=4&size=64" alt="Danny Zheng"> </span> <span class=md-profile__description> <strong> Danny Zheng </strong> <br> AI Crafter </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-02-21 00:00:00+00:00" class=md-ellipsis>2025/02/21</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=../../../../category/ai-engineering/ >AI Engineering</a>, <a href=../../../../category/prompt-engineering/ >Prompt Engineering</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 27 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <a href=edit/main/docs/posts/my_blogs/prompt-engineering.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg> </a> <a href=raw/main/docs/posts/my_blogs/prompt-engineering.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5"/></svg> </a> <h2 id=content>Content<a class=headerlink href=#content title="Permanent link">&para;</a></h2> <ul> <li><a href=#what-is-prompt-engineering>What is prompt engineering?</a></li> <li><a href=#why-prompt-engineering-matters>Why prompt engineering matters?</a></li> <li><a href=#basics-of-prompt-engineering>Basics of prompt engineering</a></li> <li><a href=#the-prompt-engineering-lifecycle>The prompt engineering lifecycle</a></li> <li><a href=#inference-parameters>Inference Parameters</a></li> <li><a href=#zero-shot>Zero-Shot</a></li> <li><a href=#few-shot>Few-Shot</a></li> <li><a href=#chain-of-thought>Chain of thought</a></li> <li><a href=#self-consistency>Self-Consistency</a></li> <li><a href=#general-tips-for-designing-prompts>General Tips for Designing Prompts</a></li> <li><a href=#common-misconceptions-about-prompts>Common Misconceptions About Prompts</a></li> <li><a href=#prompt-generator-tools>Prompt Generator Tools</a></li> <li><a href=#reasoning-model-ie-openai-o1deepseek-r1>Reasoning Model (i.e. OpenAI o1/DeepSeek R1)</a></li> </ul> <p><br> <strong>Key Takeaways:</strong><br> Prompt engineering is an ùó∂ùòÅùó≤ùóøùóÆùòÅùó∂ùòÉùó≤ ùóΩùóøùóºùó∞ùó≤ùòÄùòÄ involving continuous ùòÅùó≤ùòÄùòÅùó∂ùóªùó¥, ùó∫ùóºùó±ùó∂ùó≥ùó∂ùó∞ùóÆùòÅùó∂ùóºùóª, ùóÆùóªùó± ùóºùóΩùòÅùó∂ùó∫ùó∂ùòáùóÆùòÅùó∂ùóºùóª. As models continue to evolve, prompts must adapt accordingly to maintain effectiveness.</p> <p><br></p> <h1 id=what-is-prompt-engineering>What is prompt engineering?<a class=headerlink href=#what-is-prompt-engineering title="Permanent link">&para;</a></h1> <p>Prompt engineering is about "communicating" with LLM in a way that maximizes the model's understanding and performance on a given task. At its core, prompt engineering involves designing, refining, and optimizing the text inputs (prompts) given to models to elicit accurate, relevant, and useful responses.</p> <!-- more --> <p><br></p> <h1 id=why-prompt-engineering-matters><a href=https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/ >Why prompt engineering matters?</a><a class=headerlink href=#why-prompt-engineering-matters title="Permanent link">&para;</a></h1> <ul> <li><strong>Enhancing AI capabilities:</strong> Well-engineered prompts can dramatically improve an AI's performance, enabling it to tackle complex tasks with greater accuracy and efficiency.</li> <li><strong>Bridging the gap between human intent and AI output:</strong> Prompt engineering helps translate human objectives into language that AI models can effectively interpret and act upon.</li> <li><strong>Optimizing resource usage:</strong> Skilled prompt engineering can reduce token usage, lowering costs and improving response times in production environments.</li> </ul> <p>Let's look at two examples that demonstrate how prompt engineering can significantly enhance the performance of large language models (LLMs)</p> <h3 id=1-medprompt-a-prompting-technique-from-a-study-by-microsoft-enhances-the-performance-of-gpt-4>1. Medprompt (<a href=https://arxiv.org/pdf/2311.16452>a prompting technique from a study by Microsoft</a>) enhances the performance of GPT-4:<a class=headerlink href=#1-medprompt-a-prompting-technique-from-a-study-by-microsoft-enhances-the-performance-of-gpt-4 title="Permanent link">&para;</a></h3> <p>MedPrompt is composed of the following prompting techniques: </p> <ul> <li>Dynamic few-shot selection: instead of using static few-shot examples, Medprompt selects few-shot examples dynamically based on the question. </li> <li>Self-generated chain of thought. </li> <li>Choice shuffle ensembling: performs choice shuffle and self-consistency prompting. </li> </ul> <blockquote> <p>We will explore Few-Shot, Chain of Thought, and Self-Consistency in the following sections.</p> </blockquote> <p><img alt src=./img/Medqa-comp.png></p> <p align=center><em>Response accuracy increases significantly when applying more advanced prompt engineering techniques.</em></p> <h4 id=medprompt-allows-gpt-4-to-compete-with-fine-tuned-models-gpt-4-vs-fine-tuning>MedPrompt allows GPT-4 to compete with fine-tuned models. (GPT-4 vs Fine-tuning)<a class=headerlink href=#medprompt-allows-gpt-4-to-compete-with-fine-tuned-models-gpt-4-vs-fine-tuning title="Permanent link">&para;</a></h4> <ul> <li>While fine-tuning can boost performance, the process can be expensive. Fine-tuning often requires experts or professionally labeled datasets (e.g., via top clinicians in the MedPaLM project) and then computing model parameter updates. The process can be resource-intensive and cost-prohibitive, making the approach a difficult challenge for many small and medium-sized organizations. </li> </ul> <p><img alt src=./img/medprompt_v1.png></p> <p align=center><em>The Medprompt shows GPT-4‚Äôs ability to compete a leading model that was fine-tuned specifically for medical applications, on the same benchmarks and by a significant margin.</em></p> <h3 id=2-boosting-lower-tier-models-with-effective-prompting-gpt-35-vs-gpt-4>2. Boosting Lower-Tier Models with Effective Prompting. (GPT-3.5 vs GPT-4)<a class=headerlink href=#2-boosting-lower-tier-models-with-effective-prompting-gpt-35-vs-gpt-4 title="Permanent link">&para;</a></h3> <p>By wraping in an <strong>iterative agent workflow</strong>, GPT-3.5 achieves up to 95.1% of GPT-4 on tasks, like, content summarization and translation. For example, we can ask the LLM to iterate over a document many times: (from <a href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io">Andrew Ng's post</a> &amp; - <a href="https://www.youtube.com/watch?v=sal78ACtGTc">What's next for AI agentic workflows ft. Andrew Ng of AI Fund - 2024</a> ) </p> <ul> <li>Plan an outline.</li> <li>Write a first draft.</li> <li>Read over the first draft to spot unjustified arguments or extraneous information.</li> <li>Revise the draft taking into account any weaknesses spotted.</li> </ul> <h4 id=you-may-ask-why-not-always-use-the-most-advanced-models>You may ask, why not always use the most advanced models?<a class=headerlink href=#you-may-ask-why-not-always-use-the-most-advanced-models title="Permanent link">&para;</a></h4> <ul> <li>Cost: The advanced models are more expensive to run.</li> <li>Speed: The advanced models are slower to generate responses.</li> <li>Availability: The advanced models might not be available in certain scenarios‚Äîfor example, on edge devices. </li> </ul> <p><br></p> <h1 id=basics-of-prompt-engineering>Basics of prompt engineering<a class=headerlink href=#basics-of-prompt-engineering title="Permanent link">&para;</a></h1> <p>A prompt contains any of the following elements:‚Äã</p> <ul> <li><strong>Instruction:</strong> a specific task or instruction you want the model to perform‚Äã.</li> <li><strong>Context:</strong> external information or additional context that can steer the model to better responses‚Äã.</li> <li><strong>Input Data:</strong> the input or question that we are interested to find a response for‚Äã.</li> <li><strong>Output Indicator:</strong> the type or format of the output.‚Äã</li> </ul> <p>You do not need all the four elements for a prompt and the format depends on the task at hand. We will touch on more concrete examples in upcoming guides.‚Äã</p> <p><img alt src=./img/prompt_components.png></p> <p><br></p> <h1 id=the-prompt-engineering-lifecycle>The prompt engineering lifecycle<a class=headerlink href=#the-prompt-engineering-lifecycle title="Permanent link">&para;</a></h1> <p>It would be nice to sit down at a blank page and craft the perfect prompt on the first try, but the reality is that <strong>prompt engineering is an iterative process that involves creating, testing, and refining prompts to achieve optimal performance.</strong></p> <p>Understanding this lifecycle is crucial for developing effective prompts and troubleshooting issues that arise. </p> <ol> <li>Initial prompt creation </li> <li>Testing and identifying issues</li> <li>Selecting appropriate techniques</li> <li>Implementing improvements</li> <li>Iterating and refining</li> </ol> <p><img alt src=./img/prompt_eng_lifecycle.png></p> <p><br></p> <h1 id=inference-parameters><a href=https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#request-body>Inference Parameters</a><a class=headerlink href=#inference-parameters title="Permanent link">&para;</a></h1> <p>Inference parameters are used to control the behavior of the model during inference. There are more inference parameters, we only cover the most common ones here. </p> <ul> <li> <p><strong>System prompt:</strong> A system prompt is a way to <strong>provide role playing, context, instructions, and few-shot to LLM</strong>, while putting a question or task in the "User" turn. </p> <ul> <li><strong>Higher priority:</strong> System messages define the primary behavior and are less likely to be overridden by a later user message.</li> <li><strong>Consistency:</strong> If your application always needs certain examples or guidelines, placing them in the system prompt ensures they remain in effect throughout the conversation.</li> </ul> </li> <li> <p><strong>Max tokens:</strong> Set a limit on the number of tokens per model response. </p> <ul> <li>Set appropriate output limits. Use the max_tokens parameter to set a hard limit on the maximum length of the generated response. This prevents LLM from generating overly long outputs, which reduces latency.</li> <li>Note: Some LLMs' (e.g. Phi) max tokens = input tokens + output tokens, please be aware. <blockquote> <p>One token is roughly 4 characters for typical English text.</p> </blockquote> </li> </ul> </li> <li> <p><strong>Temperature:</strong> Controls randomness. Lowering the temperature means that the model will produce more focused and deterministic responses. Increasing the temperature will result in more diverse and creative responses. Try adjusting temperature or Top P but not both.</p> </li> <li> <p><strong>Top P:</strong> Similar to temperature, this controls randomness but uses a different method. Lowering Top P will narrow the model‚Äôs token selection to likelier tokens. Increasing Top P will let the model choose from tokens with both high and low likelihood. Try adjusting temperature or Top P but not both.</p> </li> <li> <p><strong>Frequency penalty:</strong> This decreases the likelihood of repeating the exact same text in a response.</p> <ul> <li>OpenAI models' range: -2.0 to 2.0, default value is 0.</li> </ul> </li> <li> <p><strong>Presence penalty:</strong> This increases the likelihood of introducing new topics in a response.</p> <ul> <li>OpenAI models' range: -2.0 to 2.0, default value is 0.</li> </ul> </li> <li> <p><strong>Stop sequence:</strong> Make the model end its response at a desired point. The model response will end before the specified sequence, so it won't contain the stop sequence text. For ChatGPT, using &lt;|im_end|&gt; ensures that the model response doesn't generate a follow-up user query. You can include as many as four stop sequences.</p> </li> </ul> <h1 id=lets-look-at-some-of-the-most-commonly-used-prompting-techniques>Let's look at some of the most commonly used prompting techniques:<a class=headerlink href=#lets-look-at-some-of-the-most-commonly-used-prompting-techniques title="Permanent link">&para;</a></h1> <h2 id=zero-shot>Zero-Shot<a class=headerlink href=#zero-shot title="Permanent link">&para;</a></h2> <p>Zero-shot is to simply feed the task text to the model and ask for results. <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>Text: i&#39;ll bet the video game is a lot more fun than the film.
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>Sentiment:
</span></code></pre></div></p> <h2 id=few-shot>Few-Shot<a class=headerlink href=#few-shot title="Permanent link">&para;</a></h2> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/1.Few-Shot_Prompting.ipynb>Few-Shot_Prompting.ipynb</a> </p> <p>You might also encounter the phrase "n-shot" or "one-shot". The number of "shots" refers to how many examples are used within the prompt.</p> <p><strong>Giving LLM examples of how you want it to behave (or how you want it not to behave) is extremely effective</strong> for:</p> <ul> <li>Getting the right answer</li> <li>Getting the answer in the right format (e.g. JSON, HTML, etc.). <ul> <li>Few-shot prompting is an effective way to obtain a JSON output when function calling or JSON mode is not supported by an LLM.</li> </ul> </li> </ul> <h3 id=for-maximum-effectiveness-make-sure-that-your-examples-are>For maximum effectiveness, make sure that your examples are:<a class=headerlink href=#for-maximum-effectiveness-make-sure-that-your-examples-are title="Permanent link">&para;</a></h3> <ul> <li><strong>Relevant:</strong> Your examples mirror your actual use case.</li> <li><strong>Diverse:</strong> Your examples cover edge cases and potential challenges, and vary enough that LLM doesn't inadvertently pick up on unintended patterns.</li> <li><strong>Clear:</strong> Your examples are wrapped in <example> tags (if multiple, nested within <examples> tags) for structure.</li> </ul> <p>No Examples: <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>Analyze this customer feedback and categorize the issues. Use these categories: UI/UX, Performance, Feature Request, Integration, Pricing, and Other. Also rate the sentiment (Positive/Neutral/Negative) and priority (High/Medium/Low).
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>Here is the feedback: {{FEEDBACK}}
</span></code></pre></div> With Examples: <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>Our CS team is overwhelmed with unstructured feedback. Your task is to analyze feedback and categorize issues for our product and engineering teams. Use these categories: UI/UX, Performance, Feature Request, Integration, Pricing, and Other. Also rate the sentiment (Positive/Neutral/Negative) and priority (High/Medium/Low). Here is an example:
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>&lt;example&gt;
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>Input: The new dashboard is a mess! It takes forever to load, and I can‚Äôt find the export button. Fix this ASAP!
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>Category: UI/UX, Performance
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>Sentiment: Negative
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>Priority: High
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>&lt;/example&gt;
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>Now, analyze this feedback: {{FEEDBACK}}
</span></code></pre></div></p> <h2 id=chain-of-thought>Chain of thought<a class=headerlink href=#chain-of-thought title="Permanent link">&para;</a></h2> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/3.Chain_of_Thought.ipynb>Chain_of_Thought.ipynb</a> </p> <p>Giving LLM space to think can dramatically improve its performance. This technique, known as chain of thought (CoT) prompting, encourages LLM to break down problems step-by-step, leading to more accurate and nuanced outputs. ‚Äã</p> <h4 id=why-let-llm-think>Why let LLM think?<a class=headerlink href=#why-let-llm-think title="Permanent link">&para;</a></h4> <ul> <li><strong>Accuracy:</strong> Stepping through problems reduces errors, especially in math, logic, analysis, or generally complex tasks.</li> <li><strong>Coherence:</strong> Structured thinking leads to more cohesive, well-organized responses.</li> <li><strong>Debugging:</strong> Seeing LLM‚Äôs thought process helps you pinpoint where prompts may be unclear. ‚Äã</li> </ul> <h4 id=why-not-let-llm-think>Why not let LLM think?<a class=headerlink href=#why-not-let-llm-think title="Permanent link">&para;</a></h4> <p>Increased output length may impact latency. Not all tasks require in-depth thinking. Use CoT judiciously to ensure the right balance of performance and latency.</p> <blockquote> <p>Use CoT for tasks that a human would need to think through, like complex math, multi-step analysis, writing complex documents, or decisions with many factors.</p> </blockquote> <h4 id=how-to-prompt-for-thinking>How to prompt for thinking<a class=headerlink href=#how-to-prompt-for-thinking title="Permanent link">&para;</a></h4> <blockquote> <p>CoT tip: Always have LLM output its thinking. Without outputting its thought process, no thinking occurs!</p> </blockquote> <ul> <li>Basic prompt: Include ‚ÄúThink step-by-step‚Äù in your prompt.<ul> <li>Lacks guidance on how to think (which is especially not ideal if a task is very specific to your app, use case, or organization) <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>Draft personalized emails to donors asking for contributions to this year‚Äôs Care for Kids program.
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>Program information:
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>&lt;program&gt;{{PROGRAM_DETAILS}}
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>&lt;/program&gt;
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>Donor information:
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>&lt;donor&gt;{{DONOR_DETAILS}}
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>&lt;/donor&gt;
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>Think step-by-step before you write the email.
</span></code></pre></div></li> </ul> </li> <li>Guided prompt: Outline specific steps for LLM to follow in its thinking process.<ul> <li>Lacks structuring to make it easy to strip out and separate the answer from the thinking. <div class="language-text highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>Draft personalized emails to donors asking for contributions to this year‚Äôs Care for Kids program.
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>Program information:
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>&lt;program&gt;{{PROGRAM_DETAILS}}
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>&lt;/program&gt;
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>Donor information:
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>&lt;donor&gt;{{DONOR_DETAILS}}
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a>&lt;/donor&gt;
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a>Think before you write the email. First, think through what messaging might appeal to this donor given their donation history and which campaigns they‚Äôve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email using your analysis.
</span></code></pre></div></li> </ul> </li> <li>Structured prompt: Use XML tags like <thinking> and <answer> to separate reasoning from the final answer. <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>Draft personalized emails to donors asking for contributions to this year‚Äôs Care for Kids program.
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>Program information:
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>&lt;program&gt;{{PROGRAM_DETAILS}}
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>&lt;/program&gt;
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>Donor information:
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>&lt;donor&gt;{{DONOR_DETAILS}}
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>&lt;/donor&gt;
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>Think before you write the email in &lt;thinking&gt; tags. First, think through what messaging might appeal to this donor given their donation history and which campaigns they‚Äôve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email in &lt;email&gt; tags, using your analysis.
</span></code></pre></div><blockquote> <p>Use code (e.g. extract from <code>&lt;answer&gt;</code> tags) to extract the desired answer from the LLM's response</p> </blockquote> </li> </ul> <h2 id=self-consistency>Self-Consistency<a class=headerlink href=#self-consistency title="Permanent link">&para;</a></h2> <p>Proposed by <a href=https://arxiv.org/abs/2203.11171>Wang et al. (2022)</a>, self-consistency aims "to replace the naive greedy decoding used in chain-of-thought prompting". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.</p> <h2 id=more-prompting-techiques-if-you-are-interested>More <a href=https://www.promptingguide.ai/techniques/tot>prompting techiques</a> if you are interested.<a class=headerlink href=#more-prompting-techiques-if-you-are-interested title="Permanent link">&para;</a></h2> <ul> <li><a href=https://www.promptingguide.ai/techniques/tot>Tree of Thought</a></li> <li><a href=https://www.promptingguide.ai/techniques/react>ReAct</a></li> <li><a href=https://www.promptingguide.ai/techniques/reflexion>Reflexion</a></li> <li>...</li> </ul> <p><br></p> <h1 id=general-tips-for-designing-prompts>General Tips for Designing Prompts<a class=headerlink href=#general-tips-for-designing-prompts title="Permanent link">&para;</a></h1> <p>Okey, we have covered the basics of prompt engineering. Now, let's dive into some general tips for designing prompts.</p> <h3 id=1-start-simple-source>1. Start Simple: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#1-start-simple-source title="Permanent link">&para;</a></h3> <ul> <li>As you get started with designing prompts, you should keep in mind that it is really an iterative process that requires a lot of experimentation to get optimal results. Using a simple playground, for example, Azure AI Foundry is a good starting point.</li> </ul> <h3 id=2-be-clear-direct-and-detailed-source>2. Be clear, direct, and detailed: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#2-be-clear-direct-and-detailed-source title="Permanent link">&para;</a></h3> <p>Think of LLM like any other human that is new to the job. <strong>LLM has no context</strong> on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to LLM, the better and more accurate LLM's response will be."</p> <h4 id=when-in-doubt-follow-the-golden-rule-of-clear-prompting>When in doubt, follow the <strong>Golden Rule of Clear Prompting</strong>:<a class=headerlink href=#when-in-doubt-follow-the-golden-rule-of-clear-prompting title="Permanent link">&para;</a></h4> <ul> <li>Show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the result you want. If they're confused, LLM's confused. </li> </ul> <h4 id=how-to-be-clear-contextual-and-specific>How to be clear, contextual, and specific:<a class=headerlink href=#how-to-be-clear-contextual-and-specific title="Permanent link">&para;</a></h4> <ul> <li>What the task results will be used for</li> <li>What audience the output is meant for</li> <li>What workflow the task is a part of, and where this task belongs in that workflow</li> <li>The end goal of the task, or what a successful task completion looks like</li> <li>Be specific about what you want LLM to do:<br> For example, if you want LLM to output only code and nothing else, say so.</li> <li>Provide instructions as sequential steps:<br> Use numbered lists or bullet points to better ensure that LLM carries out the task the exact way you want it to.</li> </ul> <h4 id=unclear-prompt>Unclear Prompt<a class=headerlink href=#unclear-prompt title="Permanent link">&para;</a></h4> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>Please remove all personally identifiable information from these customer feedback messages: {{FEEDBACK_DATA}}
</span></code></pre></div> <h4 id=clear-prompt>Clear Prompt<a class=headerlink href=#clear-prompt title="Permanent link">&para;</a></h4> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>Your task is to anonymize customer feedback for our quarterly review.
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>Instructions:
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>1. Replace all customer names with ‚ÄúCUSTOMER_[ID]‚Äù (e.g., ‚ÄúJane Doe‚Äù ‚Üí ‚ÄúCUSTOMER_001‚Äù).
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>2. Replace email addresses with ‚ÄúEMAIL_[ID]@example.com‚Äù.
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>3. Redact phone numbers as ‚ÄúPHONE_[ID]‚Äú.
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>4. If a message mentions a specific product (e.g., ‚ÄúAcmeCloud‚Äù), leave it intact.
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a>5. If no PII is found, copy the message verbatim.
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a>6. Output only the processed messages, separated by ‚Äù---‚Äù.
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a>
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a>Data to process: {{FEEDBACK_DATA}}
</span></code></pre></div> <h3 id=3-giving-llm-a-role-with-a-system-prompt-source>3. Giving LLM a role with a system prompt: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#3-giving-llm-a-role-with-a-system-prompt-source title="Permanent link">&para;</a></h3> <ul> <li><strong>Enhanced accuracy:</strong> In complex scenarios like legal analysis or financial modeling, role prompting can significantly boost LLM‚Äôs performance.</li> <li><strong>Tailored tone:</strong> Whether you need a CFO‚Äôs brevity or a copywriter‚Äôs flair, role prompting adjusts LLM‚Äôs communication style.</li> <li><strong>Improved focus:</strong> By setting the role context, LLM stays more within the bounds of your task‚Äôs specific requirements.<blockquote> <p>Role prompting tip: Experiment with roles! A data scientist might see different insights than a marketing strategist for the same data. A data scientist specializing in customer insight analysis for Fortune 500 companies might yield different results still!</p> </blockquote> </li> </ul> <h4 id=financial-analysis-without-role-prompting>Financial analysis without role prompting.<a class=headerlink href=#financial-analysis-without-role-prompting title="Permanent link">&para;</a></h4> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>Analyze this dataset of our Q2 financials:
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>&lt;data&gt;
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>{{FINANCIALS}}
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>&lt;/data&gt;
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>Highlight key trends and recommend actions.
</span></code></pre></div> <h4 id=financial-analysis-with-role-prompting>Financial analysis with role prompting.<a class=headerlink href=#financial-analysis-with-role-prompting title="Permanent link">&para;</a></h4> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>You are the CFO of a high-growth B2B SaaS company. We‚Äôre in a board meeting discussing our Q2 financials:
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>&lt;data&gt;
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>{{FINANCIALS}}
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>&lt;/data&gt;
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>Analyze key trends, flag concerns, and recommend strategic actions. Our investors want aggressive growth but are wary of our burn rate.
</span></code></pre></div> <h3 id=4-put-instructions-at-the-beginning-of-the-prompt-and-use-delimiters-like-or-or-xml-tags-to-separate-the-instruction-and-context-source>4. Put instructions at the beginning of the prompt and use delimiters like, <code>###</code> or <code>"""</code>, or XML tags to separate the instruction and context. (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#4-put-instructions-at-the-beginning-of-the-prompt-and-use-delimiters-like-or-or-xml-tags-to-separate-the-instruction-and-context-source title="Permanent link">&para;</a></h3> <h3 id=5-be-specific-descriptive-and-as-detailed-as-possible-about-the-desired-context-outcome-length-format-style-etc-source>5. Be specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc. (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#5-be-specific-descriptive-and-as-detailed-as-possible-about-the-desired-context-outcome-length-format-style-etc-source title="Permanent link">&para;</a></h3> <p>Be specific about the context, outcome, length, format, style, etc.</p> <p>Less effective ‚ùå: <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>Write a poem about OpenAI. 
</span></code></pre></div></p> <p>Better ‚úÖ: <div class="language-text highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a {famous poet}
</span></code></pre></div></p> <h3 id=6-reduce-fluffy-and-imprecise-descriptions-source>6. Reduce ‚Äúfluffy‚Äù and imprecise descriptions: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#6-reduce-fluffy-and-imprecise-descriptions-source title="Permanent link">&para;</a></h3> <p>Less effective ‚ùå: <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>The description for this product should be fairly short, a few sentences only, and not too much more.
</span></code></pre></div></p> <p>Better ‚úÖ: <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>Use a 3 to 5 sentence paragraph to describe this product.
</span></code></pre></div></p> <h3 id=7-instead-of-just-saying-what-not-to-do-say-what-to-do-instead-source>7. Instead of just saying what not to do, say what to do instead: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#7-instead-of-just-saying-what-not-to-do-say-what-to-do-instead-source title="Permanent link">&para;</a></h3> <p>Less effective ‚ùå: <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>The following is a conversation between an Agent and a Customer. DO NOT ASK USERNAME OR PASSWORD. DO NOT REPEAT.
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>Customer: I can‚Äôt log in to my account.
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>Agent:
</span></code></pre></div></p> <p>Better ‚úÖ: <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>Customer: I can‚Äôt log in to my account.
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>Agent:
</span></code></pre></div></p> <h3 id=8-code-generation-specific-use-leading-words-to-nudge-the-model-toward-a-particular-pattern-source>8. Code Generation Specific - Use ‚Äúleading words‚Äù to nudge the model toward a particular pattern: (<a href=https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>source</a>)<a class=headerlink href=#8-code-generation-specific-use-leading-words-to-nudge-the-model-toward-a-particular-pattern-source title="Permanent link">&para;</a></h3> <p>Less effective ‚ùå: <div class="language-text highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a># Write a simple python function that
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a># 1. Ask me for a number in mile
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a># 2. It converts miles to kilometers
</span></code></pre></div></p> <p>In this code example below, adding ‚Äúimport‚Äù hints to the model that it should start writing in Python. (Similarly ‚ÄúSELECT‚Äù is a good hint for the start of a SQL statement.) </p> <p>Better ‚úÖ: <div class="language-text highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a># Write a simple python function that
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a># 1. Ask me for a number in mile
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a># 2. It converts miles to kilometers
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>import
</span></code></pre></div></p> <h3 id=9-use-prompt-templates>9. Use prompt templates<a class=headerlink href=#9-use-prompt-templates title="Permanent link">&para;</a></h3> <p>You should always use prompt templates and variables when you expect any part of your prompt to be repeated in another call to LLM. Prompt templates offer several benefits:</p> <ul> <li>Consistency: Ensure a consistent structure for your prompts across multiple interactions</li> <li>Efficiency: Easily swap out variable content without rewriting the entire prompt</li> <li>Testability: Quickly test different inputs and edge cases by changing only the variable portion</li> <li>Scalability: Simplify prompt management as your application grows in complexity</li> <li>Version control: Easily track changes to your prompt structure over time by keeping tabs only on the core part of your prompt, separate from dynamic inputs</li> </ul> <div class="language-text highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a>Translate this text from English to Spanish: {{text}}
</span></code></pre></div> <h3 id=10-long-context-prompting-tips>10. Long context prompting tips<a class=headerlink href=#10-long-context-prompting-tips title="Permanent link">&para;</a></h3> <ul> <li><strong>Put longform data at the top:</strong> Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above your query, instructions, and examples. This can significantly improve LLM's performance.</li> </ul> <blockquote> <p>Queries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.</p> </blockquote> <ul> <li> <p><strong>Structure document content and metadata with XML tags:</strong> When using multiple documents, wrap each document in <document> tags with <document_content> and <source> (and other metadata) subtags for clarity. <div class="language-xml highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=nt>&lt;documents&gt;</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=w>  </span><span class=nt>&lt;document</span><span class=w> </span><span class=na>index=</span><span class=s>&quot;1&quot;</span><span class=nt>&gt;</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a><span class=w>    </span><span class=nt>&lt;source&gt;</span>annual_report_2023.pdf<span class=nt>&lt;/source&gt;</span>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a><span class=w>    </span><span class=nt>&lt;document_content&gt;</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a><span class=w>      </span>{{ANNUAL_REPORT}}
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a><span class=w>    </span><span class=nt>&lt;/document_content&gt;</span>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a><span class=w>  </span><span class=nt>&lt;/document&gt;</span>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a><span class=w>  </span><span class=nt>&lt;document</span><span class=w> </span><span class=na>index=</span><span class=s>&quot;2&quot;</span><span class=nt>&gt;</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a><span class=w>    </span><span class=nt>&lt;source&gt;</span>competitor_analysis_q2.xlsx<span class=nt>&lt;/source&gt;</span>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a><span class=w>    </span><span class=nt>&lt;document_content&gt;</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a><span class=w>      </span>{{COMPETITOR_ANALYSIS}}
</span><span id=__span-19-12><a id=__codelineno-19-12 name=__codelineno-19-12 href=#__codelineno-19-12></a><span class=w>    </span><span class=nt>&lt;/document_content&gt;</span>
</span><span id=__span-19-13><a id=__codelineno-19-13 name=__codelineno-19-13 href=#__codelineno-19-13></a><span class=w>  </span><span class=nt>&lt;/document&gt;</span>
</span><span id=__span-19-14><a id=__codelineno-19-14 name=__codelineno-19-14 href=#__codelineno-19-14></a><span class=nt>&lt;/documents&gt;</span>
</span><span id=__span-19-15><a id=__codelineno-19-15 name=__codelineno-19-15 href=#__codelineno-19-15></a>
</span><span id=__span-19-16><a id=__codelineno-19-16 name=__codelineno-19-16 href=#__codelineno-19-16></a>Analyze<span class=w> </span>the<span class=w> </span>annual<span class=w> </span>report<span class=w> </span>and<span class=w> </span>competitor<span class=w> </span>analysis.<span class=w> </span>Identify<span class=w> </span>strategic<span class=w> </span>advantages<span class=w> </span>and<span class=w> </span>recommend<span class=w> </span>Q3<span class=w> </span>focus<span class=w> </span>areas.
</span></code></pre></div></p> </li> <li> <p><strong>Ground responses in quotes:</strong> For long document tasks, ask LLM to quote relevant parts of the documents first before carrying out its task. This helps LLM cut through the ‚Äúnoise‚Äù of the rest of the document‚Äôs contents. <div class="language-xml highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a>You<span class=w> </span>are<span class=w> </span>an<span class=w> </span>AI<span class=w> </span>physician&#39;s<span class=w> </span>assistant.<span class=w> </span>Your<span class=w> </span>task<span class=w> </span>is<span class=w> </span>to<span class=w> </span>help<span class=w> </span>doctors<span class=w> </span>diagnose<span class=w> </span>possible<span class=w> </span>patient<span class=w> </span>illnesses.
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a><span class=nt>&lt;documents&gt;</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a><span class=w>  </span><span class=nt>&lt;document</span><span class=w> </span><span class=na>index=</span><span class=s>&quot;1&quot;</span><span class=nt>&gt;</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a><span class=w>    </span><span class=nt>&lt;source&gt;</span>patient_symptoms.txt<span class=nt>&lt;/source&gt;</span>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a><span class=w>    </span><span class=nt>&lt;document_content&gt;</span>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a><span class=w>      </span>{{PATIENT_SYMPTOMS}}
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=w>    </span><span class=nt>&lt;/document_content&gt;</span>
</span><span id=__span-20-9><a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=w>  </span><span class=nt>&lt;/document&gt;</span>
</span><span id=__span-20-10><a id=__codelineno-20-10 name=__codelineno-20-10 href=#__codelineno-20-10></a><span class=w>  </span><span class=nt>&lt;document</span><span class=w> </span><span class=na>index=</span><span class=s>&quot;2&quot;</span><span class=nt>&gt;</span>
</span><span id=__span-20-11><a id=__codelineno-20-11 name=__codelineno-20-11 href=#__codelineno-20-11></a><span class=w>    </span><span class=nt>&lt;source&gt;</span>patient_records.txt<span class=nt>&lt;/source&gt;</span>
</span><span id=__span-20-12><a id=__codelineno-20-12 name=__codelineno-20-12 href=#__codelineno-20-12></a><span class=w>    </span><span class=nt>&lt;document_content&gt;</span>
</span><span id=__span-20-13><a id=__codelineno-20-13 name=__codelineno-20-13 href=#__codelineno-20-13></a><span class=w>      </span>{{PATIENT_RECORDS}}
</span><span id=__span-20-14><a id=__codelineno-20-14 name=__codelineno-20-14 href=#__codelineno-20-14></a><span class=w>    </span><span class=nt>&lt;/document_content&gt;</span>
</span><span id=__span-20-15><a id=__codelineno-20-15 name=__codelineno-20-15 href=#__codelineno-20-15></a><span class=w>  </span><span class=nt>&lt;/document&gt;</span>
</span><span id=__span-20-16><a id=__codelineno-20-16 name=__codelineno-20-16 href=#__codelineno-20-16></a><span class=w>  </span><span class=nt>&lt;document</span><span class=w> </span><span class=na>index=</span><span class=s>&quot;3&quot;</span><span class=nt>&gt;</span>
</span><span id=__span-20-17><a id=__codelineno-20-17 name=__codelineno-20-17 href=#__codelineno-20-17></a><span class=w>    </span><span class=nt>&lt;source&gt;</span>patient01_appt_history.txt<span class=nt>&lt;/source&gt;</span>
</span><span id=__span-20-18><a id=__codelineno-20-18 name=__codelineno-20-18 href=#__codelineno-20-18></a><span class=w>    </span><span class=nt>&lt;document_content&gt;</span>
</span><span id=__span-20-19><a id=__codelineno-20-19 name=__codelineno-20-19 href=#__codelineno-20-19></a><span class=w>      </span>{{PATIENT01_APPOINTMENT_HISTORY}}
</span><span id=__span-20-20><a id=__codelineno-20-20 name=__codelineno-20-20 href=#__codelineno-20-20></a><span class=w>    </span><span class=nt>&lt;/document_content&gt;</span>
</span><span id=__span-20-21><a id=__codelineno-20-21 name=__codelineno-20-21 href=#__codelineno-20-21></a><span class=w>  </span><span class=nt>&lt;/document&gt;</span>
</span><span id=__span-20-22><a id=__codelineno-20-22 name=__codelineno-20-22 href=#__codelineno-20-22></a><span class=nt>&lt;/documents&gt;</span>
</span><span id=__span-20-23><a id=__codelineno-20-23 name=__codelineno-20-23 href=#__codelineno-20-23></a>
</span><span id=__span-20-24><a id=__codelineno-20-24 name=__codelineno-20-24 href=#__codelineno-20-24></a>Find<span class=w> </span>quotes<span class=w> </span>from<span class=w> </span>the<span class=w> </span>patient<span class=w> </span>records<span class=w> </span>and<span class=w> </span>appointment<span class=w> </span>history<span class=w> </span>that<span class=w> </span>are<span class=w> </span>relevant<span class=w> </span>to<span class=w> </span>diagnosing<span class=w> </span>the<span class=w> </span>patient&#39;s<span class=w> </span>reported<span class=w> </span>symptoms.<span class=w> </span>Place<span class=w> </span>these<span class=w> </span>in<span class=w> </span><span class=nt>&lt;quotes&gt;</span><span class=w> </span>tags.<span class=w> </span>Then,<span class=w> </span>based<span class=w> </span>on<span class=w> </span>these<span class=w> </span>quotes,<span class=w> </span>list<span class=w> </span>all<span class=w> </span>information<span class=w> </span>that<span class=w> </span>would<span class=w> </span>help<span class=w> </span>the<span class=w> </span>doctor<span class=w> </span>diagnose<span class=w> </span>the<span class=w> </span>patient&#39;s<span class=w> </span>symptoms.<span class=w> </span>Place<span class=w> </span>your<span class=w> </span>diagnostic<span class=w> </span>information<span class=w> </span>in<span class=w> </span><span class=nt>&lt;info&gt;</span><span class=w> </span>tags.
</span></code></pre></div></p> </li> </ul> <h3 id=11-chain-complex-prompts>11. Chain complex prompts<a class=headerlink href=#11-chain-complex-prompts title="Permanent link">&para;</a></h3> <p>When working with complex tasks, LLM can sometimes drop the ball if you try to handle everything in a single prompt. Chain of thought (CoT) prompting is great, but what if your task has multiple distinct steps that each require in-depth thought? <strong>Breaking down complex tasks into smaller, manageable subtasks.</strong></p> <ul> <li><strong>Accuracy:</strong> Each subtask gets LLM‚Äôs full attention, reducing errors.</li> <li><strong>Clarity:</strong> Simpler subtasks mean clearer instructions and outputs.</li> <li><strong>Traceability:</strong> Easily pinpoint and fix issues in your prompt chain.</li> </ul> <h4 id=example-chained-workflows>Example chained workflows:<a class=headerlink href=#example-chained-workflows title="Permanent link">&para;</a></h4> <ul> <li>Content creation pipelines: Research ‚Üí Outline ‚Üí Draft ‚Üí Edit ‚Üí Format.</li> <li>Data processing: Extract ‚Üí Transform ‚Üí Analyze ‚Üí Visualize.</li> <li>Decision-making: Gather info ‚Üí List options ‚Üí Analyze each ‚Üí Recommend.</li> <li>Verification loops: Generate content ‚Üí Review ‚Üí Refine ‚Üí Re-review.</li> </ul> <h3 id=12-tool-use-function-calling>12. Tool use (function calling)<a class=headerlink href=#12-tool-use-function-calling title="Permanent link">&para;</a></h3> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/4.Tool_Use_Function-Calling.ipynb>Tool_Use_Function-Calling.ipynb</a> </p> <p>Function calling provides a powerful and flexible way for LLMs to interface with your code or external services, and has two primary use cases:</p> <ul> <li>Fetching Data Retrieve up-to-date information to incorporate into the model's response (RAG). Useful for searching knowledge bases and retrieving specific data from APIs (e.g. current weather data).</li> <li>Taking Action Perform actions like submitting a form, calling APIs, modifying application state (UI/frontend or backend), or taking agentic workflow actions (like handing off the conversation).</li> </ul> <p>When you define a function as part of your request, the details are injected into the system message using specific syntax that the model has been trained on. This means that functions consume tokens in your prompt and that you can apply prompt engineering techniques to optimize the performance of your function calls. The model uses the full context of the prompt to determine if a function should be called including function definition, the system message, and the user messages.</p> <h3 id=improving-tool-use-quality-and-reliability>Improving tool use quality and reliability<a class=headerlink href=#improving-tool-use-quality-and-reliability title="Permanent link">&para;</a></h3> <p>If the model isn't calling your function when or how you expect, there are a few things you can try to improve the quality.</p> <h4 id=a-provide-more-details-in-your-function-definition>a. Provide more details in your function definition<a class=headerlink href=#a-provide-more-details-in-your-function-definition title="Permanent link">&para;</a></h4> <p>It's important that you provide a meaningful description of the function and provide descriptions for any parameter that might not be obvious to the model. For example, in the description for the location parameter, you could include extra details and examples on the format of the location. <div class="language-json highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=nt>&quot;location&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=w>    </span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;string&quot;</span><span class=p>,</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a><span class=w>    </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;The location of the hotel. The location should include the city and the state&#39;s abbreviation (i.e. Seattle, WA or Miami, FL)&quot;</span>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a><span class=p>}</span>
</span></code></pre></div></p> <h4 id=b-provide-more-context-in-the-system-message>b. Provide more context in the system message<a class=headerlink href=#b-provide-more-context-in-the-system-message title="Permanent link">&para;</a></h4> <p>The system message can also be used to provide more context to the model. For example, if you have a function called search_hotels you could include a system message like the following to instruct the model to call the function when a user asks for help with finding a hotel.</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=p>{</span><span class=nt>&quot;role&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;system&quot;</span><span class=p>,</span><span class=w> </span><span class=nt>&quot;content&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;You&#39;re an AI assistant designed to help users search for hotels. When a user asks for help finding a hotel, you should call the search_hotels function.&quot;</span><span class=p>}</span>
</span></code></pre></div> <h4 id=c-instruct-the-model-to-ask-clarifying-questions>c. Instruct the model to ask clarifying questions<a class=headerlink href=#c-instruct-the-model-to-ask-clarifying-questions title="Permanent link">&para;</a></h4> <p>In some cases, you want to instruct the model to ask clarifying questions to prevent making assumptions about what values to use with functions. For example, with search_hotels you would want the model to ask for clarification if the user request didn't include details on location. To instruct the model to ask a clarifying question, you could include content like the next example in your system message. <div class="language-json highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a><span class=p>{</span><span class=nt>&quot;role&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;system&quot;</span><span class=p>,</span><span class=w> </span><span class=nt>&quot;content&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;Don&#39;t make assumptions about what values to use with functions. Ask for clarification if a user request is ambiguous.&quot;</span><span class=p>}</span>
</span></code></pre></div></p> <h4 id=d-offload-the-burden-from-the-model-and-use-code-where-possible>d. Offload the burden from the model and use code where possible.<a class=headerlink href=#d-offload-the-burden-from-the-model-and-use-code-where-possible title="Permanent link">&para;</a></h4> <ul> <li>Don't make the model fill arguments you already know. </li> <li>Combine functions that are always called in sequence. For example, if you always call <code>mark_location()</code> after <code>query_location()</code>, just move the marking logic into the query function call.</li> </ul> <h4 id=e-keep-the-number-of-functions-small-for-higher-accuracy>e. Keep the number of functions small for higher accuracy.<a class=headerlink href=#e-keep-the-number-of-functions-small-for-higher-accuracy title="Permanent link">&para;</a></h4> <ul> <li><strong>Evaluate your performance</strong> with different numbers of functions.</li> <li><strong>Aim for fewer than 20 functions</strong> at any one time, though this is just a soft suggestion.</li> </ul> <h3 id=13-increse-output-consistency-json-modestructured-output>13. Increse output consistency (JSON mode/Structured Output)<a class=headerlink href=#13-increse-output-consistency-json-modestructured-output title="Permanent link">&para;</a></h3> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/2.Structured_Output_JOSN-Mode.ipynb>Structured_Output_JOSN-Mode.ipynb</a> </p> <p>Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.</p> <p>Some benefits of Structured Outputs include:</p> <ul> <li><strong>Reliable type-safety:</strong> No need to validate or retry incorrectly formatted responses</li> <li><strong>Explicit refusals:</strong> Safety-based model refusals are now programmatically detectable</li> <li><strong>Simpler prompting:</strong> No need for strongly worded prompts to achieve consistent formatting</li> </ul> <h4 id=a-four-effective-ways-to-generate-structured-outputs>a. Four Effective Ways to Generate Structured Outputs<a class=headerlink href=#a-four-effective-ways-to-generate-structured-outputs title="Permanent link">&para;</a></h4> <p>When working with LLMs, ensuring <strong>structured outputs</strong> (such as JSON) can improve reliability and make parsing easier. Here are four key methods to achieve this: </p> <ol> <li> <p><strong>Use Few-Shot Examples</strong> </p> <ul> <li>Provide examples of the desired <strong>JSON output</strong> directly in the prompt to guide the model. </li> <li>Particularly useful for models <strong>without native JSON mode or function calling</strong> support. </li> </ul> </li> <li> <p><strong>Leverage <a href=https://platform.openai.com/docs/guides/structured-outputs#json-mode>JSON Mode</a></strong> </p> <ul> <li>If supported, enable JSON mode to ensure outputs are <strong>strictly formatted</strong> as JSON. </li> </ul> </li> <li> <p><strong>Utilize <a href=https://platform.openai.com/docs/guides/function-calling>Function Calling</a></strong> </p> <ul> <li>Define structured functions that the model can call, ensuring outputs adhere to a predefined schema. </li> </ul> </li> <li> <p><strong>OpenAI‚Äôs new <a href=https://platform.openai.com/docs/guides/structured-outputs>Structured Outputs</a> feature</strong> </p> <ul> <li>OpenAI provides tools for generating well-structured responses, reducing errors in format consistency. </li> </ul> </li> </ol> <h4 id=b-when-to-use-structured-outputs-via-function-calling-vs-via-response_format>b. When to use Structured Outputs via function calling vs via response_format<a class=headerlink href=#b-when-to-use-structured-outputs-via-function-calling-vs-via-response_format title="Permanent link">&para;</a></h4> <p>Structured Outputs is available in two forms in the OpenAI API:</p> <ul> <li>When using function calling</li> <li>When using a json_schema response format</li> </ul> <p>Function calling is useful when you are building an application that bridges the models and functionality of your application.</p> <p>For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.</p> <p>Conversely, Structured Outputs via response_format are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.</p> <p>Put simply:</p> <p>If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling.</p> <p>If you want to structure the model's output when it responds to the user, then you should use a structured response_format.</p> <h4 id=c-structured-outputs-vs-json-mode>c. Structured Outputs vs JSON mode<a class=headerlink href=#c-structured-outputs-vs-json-mode title="Permanent link">&para;</a></h4> <p>Structured Outputs is the evolution of JSON mode. While both ensure valid JSON is produced, only <strong>Structured Outputs ensure schema adherance</strong>. </p> <p><strong>Always using Structured Outputs instead of JSON mode when possible.</strong></p> <p>However, Structured Outputs with response_format: <code>{type: "json_schema", ...}</code> is only supported with the <code>gpt-4o-mini</code>, <code>gpt-4o-mini-2024-07-18</code>, and <code>gpt-4o-2024-08-06</code> model snapshots and later.</p> <h3 id=14-reducing-latency>14. Reducing Latency<a class=headerlink href=#14-reducing-latency title="Permanent link">&para;</a></h3> <p>Latency can be influenced by various factors, such as the size of the model, the complexity of the prompt, and the underlying infrastucture supporting the model and point of interaction.</p> <blockquote> <p>It‚Äôs always better to first engineer a prompt that works well without model or prompt constraints, and then try latency reduction strategies afterward. Trying to reduce latency prematurely might prevent you from discovering what top performance looks like.</p> </blockquote> <h4 id=how-to-reduce-latency>How to reduce latency:<a class=headerlink href=#how-to-reduce-latency title="Permanent link">&para;</a></h4> <p>a. Choose the right model.<br> b. Optimize prompt to use fewer input tokens: </p> <ul> <li>Be clear but concise.</li> <li>Fine-tuning the model, to replace the need for lengthy instructions / examples.</li> <li>Filtering context input, like pruning RAG results, cleaning HTML, etc.</li> <li>Maximize shared prompt prefix, by putting dynamic portions (e.g. RAG results, history, etc) later in the prompt. This makes your request more KV cache-friendly (which most LLM providers use) and means fewer input tokens are processed on each request. <ul> <li><a href=https://platform.openai.com/docs/guides/prompt-caching>prompt-caching</a>: structure prompts with static or repeated content at the beginning and dynamic content at the end. </li> </ul> </li> </ul> <p>c. Generate fewer tokens:<br> Generating tokens is almost always the highest latency step when using an LLM: as a general heuristic, cutting 50% of your output tokens may cut ~50% your latency. </p> <ul> <li>Ask for shorter responses.</li> <li>Set appropriate output limits. Use the max_tokens parameter to set a hard limit on the maximum length of the generated response. This prevents LLM from generating overly long outputs.</li> <li>Use <code>stop_tokens</code> to end your generation early.</li> <li>Experiment with temperature: The temperature parameter controls the randomness of the output. Lower values (e.g., 0.2) can sometimes lead to more focused and shorter responses, while higher values (e.g., 0.8) may result in more diverse but potentially longer outputs.</li> </ul> <p>d. Process tokens faster. </p> <ul> <li>Using a longer, more detailed prompt,</li> <li>Adding (more) few-shot examples, or</li> <li>Fine-tuning / distillation.</li> <li><a href=https://platform.openai.com/docs/guides/predicted-outputs>Predicted output</a> (OpenAI API) <div class="language-js highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=w>  </span><span class=kd>const</span><span class=w> </span><span class=nx>code</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=sb>`</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a><span class=sb>  class User {</span>
</span><span id=__span-24-3><a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a><span class=sb>    firstName: string = &quot;&quot;;</span>
</span><span id=__span-24-4><a id=__codelineno-24-4 name=__codelineno-24-4 href=#__codelineno-24-4></a><span class=sb>    lastName: string = &quot;&quot;;</span>
</span><span id=__span-24-5><a id=__codelineno-24-5 name=__codelineno-24-5 href=#__codelineno-24-5></a><span class=sb>    username: string = &quot;&quot;;</span>
</span><span id=__span-24-6><a id=__codelineno-24-6 name=__codelineno-24-6 href=#__codelineno-24-6></a><span class=sb>  }</span>
</span><span id=__span-24-7><a id=__codelineno-24-7 name=__codelineno-24-7 href=#__codelineno-24-7></a>
</span><span id=__span-24-8><a id=__codelineno-24-8 name=__codelineno-24-8 href=#__codelineno-24-8></a><span class=sb>  export default User;</span>
</span><span id=__span-24-9><a id=__codelineno-24-9 name=__codelineno-24-9 href=#__codelineno-24-9></a><span class=sb>  `</span><span class=p>.</span><span class=nx>trim</span><span class=p>();</span>
</span><span id=__span-24-10><a id=__codelineno-24-10 name=__codelineno-24-10 href=#__codelineno-24-10></a>
</span><span id=__span-24-11><a id=__codelineno-24-11 name=__codelineno-24-11 href=#__codelineno-24-11></a><span class=w>  </span><span class=kd>const</span><span class=w> </span><span class=nx>completion</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>await</span><span class=w> </span><span class=nx>openai</span><span class=p>.</span><span class=nx>chat</span><span class=p>.</span><span class=nx>completions</span><span class=p>.</span><span class=nx>create</span><span class=p>({</span>
</span><span id=__span-24-12><a id=__codelineno-24-12 name=__codelineno-24-12 href=#__codelineno-24-12></a><span class=w>    </span><span class=nx>model</span><span class=o>:</span><span class=w> </span><span class=s2>&quot;gpt-4o&quot;</span><span class=p>,</span>
</span><span id=__span-24-13><a id=__codelineno-24-13 name=__codelineno-24-13 href=#__codelineno-24-13></a><span class=w>    </span><span class=nx>messages</span><span class=o>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-24-14><a id=__codelineno-24-14 name=__codelineno-24-14 href=#__codelineno-24-14></a><span class=w>      </span><span class=p>{</span>
</span><span id=__span-24-15><a id=__codelineno-24-15 name=__codelineno-24-15 href=#__codelineno-24-15></a><span class=w>        </span><span class=nx>role</span><span class=o>:</span><span class=w> </span><span class=s2>&quot;user&quot;</span><span class=p>,</span>
</span><span id=__span-24-16><a id=__codelineno-24-16 name=__codelineno-24-16 href=#__codelineno-24-16></a><span class=w>        </span><span class=nx>content</span><span class=o>:</span><span class=w> </span><span class=nx>refactorPrompt</span>
</span><span id=__span-24-17><a id=__codelineno-24-17 name=__codelineno-24-17 href=#__codelineno-24-17></a><span class=w>      </span><span class=p>},</span>
</span><span id=__span-24-18><a id=__codelineno-24-18 name=__codelineno-24-18 href=#__codelineno-24-18></a><span class=w>      </span><span class=p>{</span>
</span><span id=__span-24-19><a id=__codelineno-24-19 name=__codelineno-24-19 href=#__codelineno-24-19></a><span class=w>        </span><span class=nx>role</span><span class=o>:</span><span class=w> </span><span class=s2>&quot;user&quot;</span><span class=p>,</span>
</span><span id=__span-24-20><a id=__codelineno-24-20 name=__codelineno-24-20 href=#__codelineno-24-20></a><span class=w>        </span><span class=nx>content</span><span class=o>:</span><span class=w> </span><span class=nx>code</span>
</span><span id=__span-24-21><a id=__codelineno-24-21 name=__codelineno-24-21 href=#__codelineno-24-21></a><span class=w>      </span><span class=p>}</span>
</span><span id=__span-24-22><a id=__codelineno-24-22 name=__codelineno-24-22 href=#__codelineno-24-22></a><span class=w>    </span><span class=p>],</span>
</span><span id=__span-24-23><a id=__codelineno-24-23 name=__codelineno-24-23 href=#__codelineno-24-23></a><span class=w>    </span><span class=nx>store</span><span class=o>:</span><span class=w> </span><span class=kc>true</span><span class=p>,</span>
</span><span id=__span-24-24><a id=__codelineno-24-24 name=__codelineno-24-24 href=#__codelineno-24-24></a><span class=w>    </span><span class=nx>prediction</span><span class=o>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-24-25><a id=__codelineno-24-25 name=__codelineno-24-25 href=#__codelineno-24-25></a><span class=w>      </span><span class=nx>type</span><span class=o>:</span><span class=w> </span><span class=s2>&quot;content&quot;</span><span class=p>,</span>
</span><span id=__span-24-26><a id=__codelineno-24-26 name=__codelineno-24-26 href=#__codelineno-24-26></a><span class=w>      </span><span class=nx>content</span><span class=o>:</span><span class=w> </span><span class=nx>code</span>
</span><span id=__span-24-27><a id=__codelineno-24-27 name=__codelineno-24-27 href=#__codelineno-24-27></a><span class=w>    </span><span class=p>}</span>
</span><span id=__span-24-28><a id=__codelineno-24-28 name=__codelineno-24-28 href=#__codelineno-24-28></a><span class=w>  </span><span class=p>});</span>
</span></code></pre></div></li> </ul> <p>e. Leverage streaming, make your users wait less. </p> <p>Streaming is a feature that allows the model to start sending back its response before the full output is complete. </p> <h3 id=15-avoiding-hallucinations>15. Avoiding hallucinations<a class=headerlink href=#15-avoiding-hallucinations title="Permanent link">&para;</a></h3> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/5.Avoiding_Hallucinations.ipynb>Avoiding_Hallucinations.ipynb</a></p> <h4 id=a-allow-llm-to-say-i-dont-know>a. Allow LLM to say ‚ÄúI don‚Äôt know‚Äù.<a class=headerlink href=#a-allow-llm-to-say-i-dont-know title="Permanent link">&para;</a></h4> <div style="font-family: monospace; background:#F5F7FA ; font-size: 14.08px; padding: 10px;"> As our M&A advisor, analyze this report on the potential acquisition of AcmeCo by ExampleCorp. <report> {{REPORT}} </report> Focus on financial projections, integration risks, and regulatory hurdles. If you‚Äôre unsure about any aspect or if the report lacks necessary information, say <span style="color: green;">‚ÄúI don‚Äôt have enough information to confidently assess this.‚Äù</span> </div> <h4 id=b-use-direct-quotes-for-factual-grounding>b. Use direct quotes for factual grounding:<a class=headerlink href=#b-use-direct-quotes-for-factual-grounding title="Permanent link">&para;</a></h4> <ul> <li>For tasks involving long documents (&gt;20K tokens), ask LLM to extract word-for-word quotes first before performing its task. This grounds its responses in the actual text, reducing hallucinations.</li> </ul> <div style="font-family: monospace; background:#F5F7FA ; font-size: 14.08px; padding: 10px;"> As our Data Protection Officer, review this updated privacy policy for GDPR and CCPA compliance. <policy> {{POLICY}} </policy> Extract exact quotes from the policy that are most relevant to GDPR and CCPA compliance. If you can‚Äôt find relevant quotes, state ‚ÄúNo relevant quotes found.‚Äù 2. <span style="color: green;">Use the quotes to analyze the compliance of these policy sections, referencing the quotes by number. Only base your analysis on the extracted quotes.</span> </div> <h4 id=c-verify-with-citations>c. Verify with citations:<a class=headerlink href=#c-verify-with-citations title="Permanent link">&para;</a></h4> <p>Make LLM‚Äôs response auditable by having it cite quotes and sources for each of its claims. You can also have LLM verify each claim by finding a supporting quote after it generates a response. If it can‚Äôt find a quote, it must retract the claim.</p> <div style="font-family: monospace; background:#F5F7FA ; font-size: 14.08px ; padding: 10px;"> Draft a press release for our new cybersecurity product, AcmeSecurity Pro, using only information from these product briefs and market reports. <documents> {{DOCUMENTS}} </documents> <div style="color: green;"> After drafting, review each claim in your press release. For each claim, find a direct quote from the documents that supports it. If you can‚Äôt find a supporting quote for a claim, remove that claim from the press release and mark where it was removed with empty [] brackets. </div> </div> <h4 id=d-advanced-techniques>d. Advanced techniques:<a class=headerlink href=#d-advanced-techniques title="Permanent link">&para;</a></h4> <ul> <li> <p><strong>Chain-of-thought verification:</strong> Ask LLM to explain its reasoning step-by-step before giving a final answer. This can reveal faulty logic or assumptions.</p> </li> <li> <p><strong>Best-of-N verficiation:</strong> Run LLM through the same prompt multiple times and compare the outputs. Inconsistencies across outputs could indicate hallucinations.</p> </li> <li> <p><strong>Iterative refinement:</strong> Use LLM‚Äôs outputs as inputs for follow-up prompts, asking it to verify or expand on previous statements. This can catch and correct inconsistencies.</p> </li> <li> <p><strong>External knowledge restriction:</strong> Explicitly instruct LLM to only use information from provided documents and not its general knowledge.</p> </li> </ul> <h3 id=16-split-complex-tasks-into-simpler-subtasks>16. Split complex tasks into simpler subtasks<a class=headerlink href=#16-split-complex-tasks-into-simpler-subtasks title="Permanent link">&para;</a></h3> <p>Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.</p> <ul> <li>Use intent classification to identify the most relevant instructions for a user query</li> <li>For dialogue applications that require very long conversations, summarize or filter previous dialogue.</li> <li>Summarize long documents piecewise and construct a full summary recursively.</li> </ul> <h3 id=17-test-changes-systematically>17. Test changes systematically<a class=headerlink href=#17-test-changes-systematically title="Permanent link">&para;</a></h3> <p>Sometimes it can be hard to tell whether a change ‚Äî e.g., a new instruction or a new design ‚Äî makes your system better or worse.<br> Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.</p> <ul> <li><strong>Evaluate model outputs with reference to gold-standard answers</strong></li> </ul> <p>Leverage prompt evaluation tools to accelerate your assessment process:</p> <ul> <li><a href=https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool>Anthropic</a></li> <li><a href=https://github.com/openai/evals>OpenAI</a></li> <li><a href=https://docs.ragas.io/en/stable/ >Ragas</a></li> <li><a href=https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-generative-ai-app>How to evaluate generative AI models and applications with Azure AI Foundry</a></li> </ul> <p><br></p> <h1 id=common-misconceptions-about-prompts>Common Misconceptions About Prompts<a class=headerlink href=#common-misconceptions-about-prompts title="Permanent link">&para;</a></h1> <h3 id=1-a-prompt-is-static-write-it-once-and-youre-done>1. A prompt is static; write it once and you‚Äôre done. ‚ùå<a class=headerlink href=#1-a-prompt-is-static-write-it-once-and-youre-done title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some think writing a prompt is like writing an article‚Äîonce you finish, it‚Äôs done, and no further changes are necessary.</li> <li>‚úÖ <strong>Reality:</strong> A prompt is actually a complex programming method, requiring the same care we apply to code, such as version control and experiment tracking. Crafting a good prompt involves careful design and iteration, ensuring the model accurately understands the task and produces the desired output. Prompt engineering is an iterative process involving continuous testing, modification, and optimization.</li> </ul> <h3 id=2-prompts-require-perfect-grammar-and-punctuation>2. Prompts require perfect grammar and punctuation. ‚ùå<a class=headerlink href=#2-prompts-require-perfect-grammar-and-punctuation title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> People assume a model only understands a prompt if it‚Äôs written in flawless grammar and punctuation.</li> <li>‚úÖ <strong>Reality:</strong> While attention to detail is important, the model can typically handle prompts with typos or imperfect grammar. Conceptual clarity matters more than perfect grammar. Although it‚Äôs good to correct errors in the final prompt, it‚Äôs fine to have minor flaws during the iterative process.</li> </ul> <h3 id=3-you-have-to-trick-the-model-into-working>3. You have to ‚Äòtrick‚Äô the model into working. ‚ùå<a class=headerlink href=#3-you-have-to-trick-the-model-into-working title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some believe the model is ‚Äúdumb‚Äù and needs tricks or ‚Äúlies‚Äù to get the job done, such as saying " I will tip you $500".</li> <li><strong>Reality:</strong> Models are quite capable. You don‚Äôt need to ‚Äútrick‚Äù them. Rather, you should respect the model and provide clear, accurate information so it understands your goal. Simply describe your task directly, rather than using metaphors or a similar task to guide the model.</li> </ul> <h3 id=4-prompt-engineering-is-all-about-crafting-a-perfect-instruction>4. Prompt engineering is all about crafting a perfect instruction. ‚ùå<a class=headerlink href=#4-prompt-engineering-is-all-about-crafting-a-perfect-instruction title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some think prompt engineering is just finding the perfect instruction, spending large amounts of time agonizing over every single word.</li> <li>‚úÖ <strong>Reality:</strong> While precise instructions do matter, <strong>it‚Äôs even more crucial to understand how the model operates and to learn from reading its outputs</strong>. Understanding the model‚Äôs reasoning‚Äîhow it processes different inputs‚Äîmatters more than chasing a so-called perfect instruction. A good prompt engineer can interpret signals from the model‚Äôs output and grasp its reasoning process, not just look at whether the result is correct.</li> </ul> <h3 id=5-prompt-engineering-is-purely-about-writing-skill>5. Prompt engineering is purely about writing skill. ‚ùå<a class=headerlink href=#5-prompt-engineering-is-purely-about-writing-skill title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some believe the main skill in prompt engineering is writing proficiency, so someone who writes well will naturally excel at it.</li> <li>‚úÖ <strong>Reality:</strong> Although strong writing skills are necessary, they‚Äôre not the core capability. <strong>Good prompt engineers need an experimental mindset, systematic thinking, problem-solving skills, and insight into how the model ‚Äúthinks.‚Äù Iteration and testing matter more than writing ability alone.</strong></li> </ul> <h3 id=6-more-examples-always-produce-better-prompts>6. More examples always produce better prompts. ‚ùå<a class=headerlink href=#6-more-examples-always-produce-better-prompts title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> People may think providing a large number of examples is the only way to improve the model‚Äôs performance.</li> <li>‚úÖ <strong>Reality:</strong> While examples can help guide the model, <strong>having too many can limit creativity and variety</strong>. In research contexts, using illustrative rather than highly specific examples can be more effective, because it encourages the model to focus on the underlying task rather than just copying examples.</li> </ul> <h3 id=7-you-should-avoid-giving-the-model-too-much-information>7. You should avoid giving the model too much information. ‚ùå<a class=headerlink href=#7-you-should-avoid-giving-the-model-too-much-information title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some worry giving the model too many details will confuse it, so they keep the instructions minimal and hide complexity.</li> <li>‚úÖ <strong>Reality:</strong> As models become more capable, they can handle more information and context. <strong>You should trust the model by giving it enough information to better understand your task.</strong></li> </ul> <h3 id=8-role-playing-prompts-always-work>8. Role-playing prompts always work. ‚ùå<a class=headerlink href=#8-role-playing-prompts-always-work title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some believe that giving the model a specific role (e.g. ‚ÄúYou are a teacher‚Äù) automatically boosts its performance.</li> <li>‚úÖ <strong>Reality:</strong> Role-playing prompts may help in certain scenarios but aren‚Äôt always necessary. Often, simply stating your task is more effective. <strong>As models improve, it may be better to give a direct task description rather than assigning a fake identity.</strong></li> </ul> <h3 id=9-once-you-find-a-good-prompt-itll-work-forever>9. Once you find a good prompt, it‚Äôll work forever. ‚ùå<a class=headerlink href=#9-once-you-find-a-good-prompt-itll-work-forever title="Permanent link">&para;</a></h3> <ul> <li><strong>Misconception:</strong> Some believe once you find an effective prompt, you can reuse it indefinitely without further changes.</li> <li>‚úÖ <strong>Reality:</strong> As models keep improving, prompts that used to work can become obsolete. <strong>Some prompting techniques might get ‚Äúbaked into‚Äù the model‚Äôs training, making them unnecessary later. You have to keep learning and adapting to changes in the model.</strong></li> </ul> <p><br></p> <h1 id=prompt-generator-tools>Prompt Generator Tools<a class=headerlink href=#prompt-generator-tools title="Permanent link">&para;</a></h1> <p>Now that you've explored various prompt engineering techniques and debunked common misconceptions, you might be wondering‚Äî<strong>how can you generate high-quality prompts effortlessly?</strong></p> <p>Following the detailed steps above can sometimes feel overwhelming, but don't worry! To make things easier, here are some prompt generator tools that can streamline the process and help you craft effective prompts with ease.</p> <h3 id=1-anthropic-prompt-generator><strong>1. Anthropic Prompt Generator</strong><a class=headerlink href=#1-anthropic-prompt-generator title="Permanent link">&para;</a></h3> <p><strong>Anthropic prompt generator</strong> is my personal favorite. I use it frequently to <strong>generate prompts</strong> and <strong>enhance existing ones</strong> for better performance. It convert a simple text into a well-structured prompt. In most cases, you can use it as is without modifying the output. </p> <p>An example input prompt: <div class="language-text highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a>Summarize an article.
</span></code></pre></div> Output prompt: <div class="language-text highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a>You are tasked with summarizing an article. Please follow these instructions carefully:
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a>
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a>1. First, read the following article in its entirety:
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a>
</span><span id=__span-26-5><a id=__codelineno-26-5 name=__codelineno-26-5 href=#__codelineno-26-5></a>&lt;article&gt;
</span><span id=__span-26-6><a id=__codelineno-26-6 name=__codelineno-26-6 href=#__codelineno-26-6></a>{{ARTICLE}}
</span><span id=__span-26-7><a id=__codelineno-26-7 name=__codelineno-26-7 href=#__codelineno-26-7></a>&lt;/article&gt;
</span><span id=__span-26-8><a id=__codelineno-26-8 name=__codelineno-26-8 href=#__codelineno-26-8></a>
</span><span id=__span-26-9><a id=__codelineno-26-9 name=__codelineno-26-9 href=#__codelineno-26-9></a>2. After reading, take a moment to analyze the article&#39;s main points, key arguments, and overall structure.
</span><span id=__span-26-10><a id=__codelineno-26-10 name=__codelineno-26-10 href=#__codelineno-26-10></a>
</span><span id=__span-26-11><a id=__codelineno-26-11 name=__codelineno-26-11 href=#__codelineno-26-11></a>3. Create a concise summary of the article that captures its essence. Your summary should:
</span><span id=__span-26-12><a id=__codelineno-26-12 name=__codelineno-26-12 href=#__codelineno-26-12></a>   - Identify the main topic or thesis of the article
</span><span id=__span-26-13><a id=__codelineno-26-13 name=__codelineno-26-13 href=#__codelineno-26-13></a>   - Highlight the key points and arguments presented
</span><span id=__span-26-14><a id=__codelineno-26-14 name=__codelineno-26-14 href=#__codelineno-26-14></a>   - Include any significant data, statistics, or examples that support the main ideas
</span><span id=__span-26-15><a id=__codelineno-26-15 name=__codelineno-26-15 href=#__codelineno-26-15></a>   - Maintain the original tone and intent of the article
</span><span id=__span-26-16><a id=__codelineno-26-16 name=__codelineno-26-16 href=#__codelineno-26-16></a>
</span><span id=__span-26-17><a id=__codelineno-26-17 name=__codelineno-26-17 href=#__codelineno-26-17></a>4. Your summary should be approximately 3-5 sentences long, or about 100-150 words. It should be written in a clear, objective style.
</span><span id=__span-26-18><a id=__codelineno-26-18 name=__codelineno-26-18 href=#__codelineno-26-18></a>
</span><span id=__span-26-19><a id=__codelineno-26-19 name=__codelineno-26-19 href=#__codelineno-26-19></a>5. After the summary, provide a brief assessment (2-3 sentences) of the article&#39;s main contributions or significance to its field or to the broader discourse.
</span><span id=__span-26-20><a id=__codelineno-26-20 name=__codelineno-26-20 href=#__codelineno-26-20></a>
</span><span id=__span-26-21><a id=__codelineno-26-21 name=__codelineno-26-21 href=#__codelineno-26-21></a>Please present your response in the following format:
</span><span id=__span-26-22><a id=__codelineno-26-22 name=__codelineno-26-22 href=#__codelineno-26-22></a>
</span><span id=__span-26-23><a id=__codelineno-26-23 name=__codelineno-26-23 href=#__codelineno-26-23></a>&lt;summary&gt;
</span><span id=__span-26-24><a id=__codelineno-26-24 name=__codelineno-26-24 href=#__codelineno-26-24></a>[Insert your 3-5 sentence summary here]
</span><span id=__span-26-25><a id=__codelineno-26-25 name=__codelineno-26-25 href=#__codelineno-26-25></a>&lt;/summary&gt;
</span><span id=__span-26-26><a id=__codelineno-26-26 name=__codelineno-26-26 href=#__codelineno-26-26></a>
</span><span id=__span-26-27><a id=__codelineno-26-27 name=__codelineno-26-27 href=#__codelineno-26-27></a>&lt;assessment&gt;
</span><span id=__span-26-28><a id=__codelineno-26-28 name=__codelineno-26-28 href=#__codelineno-26-28></a>[Insert your 2-3 sentence assessment here]
</span><span id=__span-26-29><a id=__codelineno-26-29 name=__codelineno-26-29 href=#__codelineno-26-29></a>&lt;/assessment&gt;
</span><span id=__span-26-30><a id=__codelineno-26-30 name=__codelineno-26-30 href=#__codelineno-26-30></a>
</span><span id=__span-26-31><a id=__codelineno-26-31 name=__codelineno-26-31 href=#__codelineno-26-31></a>Remember to focus on the most crucial information and maintain the original meaning of the article without inserting your own opinions or interpretations in the summary.
</span></code></pre></div></p> <p><img alt="Anthropic Prompt Generator" src=./img/anthropic_prompt_generator.png><br> üîó <a href=https://console.anthropic.com/dashboard>Try it here</a> </p> <h3 id=2-azure-ai-foundry><strong>2. Azure AI Foundry</strong><a class=headerlink href=#2-azure-ai-foundry title="Permanent link">&para;</a></h3> <p>Microsoft's <strong>Azure AI Foundry</strong> provides robust AI-powered tools to assist with prompt generation and optimization, making it a great resource for AI developers.<br> üîó <a href=https://ai.azure.com/ >Try it here</a> </p> <p><img alt="Azure AI Foundry" src=./img/azure_ai_foundry_prompt_generator.png> </p> <h3 id=3-microsoft-365-copilot><strong>3. Microsoft 365 Copilot</strong><a class=headerlink href=#3-microsoft-365-copilot title="Permanent link">&para;</a></h3> <p><strong>Microsoft 365 Copilot</strong> can assist in generating contextual prompts directly within your workflow.<br> üîó <a href="https://www.microsoft365.com/chat/?auth=2&home=1">Try it here</a> </p> <p><img alt="Microsoft 365 Copilot" src=./img/prompt_coach.png> </p> <blockquote> <p>Hands-on notebook: <a href=https://github.com/AICrafterZheng/aicrafterzheng.github.io/blob/main/docs/posts/notebooks/6.Prompt_Generation.ipynb>Prompt_Generation.ipynb</a></p> </blockquote> <p><br></p> <h1 id=reasoning-model-ie-openai-o1deepseek-r1>Reasoning Model (i.e. OpenAI o1/DeepSeek R1)<a class=headerlink href=#reasoning-model-ie-openai-o1deepseek-r1 title="Permanent link">&para;</a></h1> <p>Finally, let's explore the reasoning model, which highlights that prompt engineering is an iterative process, requiring prompts to evolve alongside model advancements. For instance, the chain of thought is no longer necessary for the reasoning model.</p> <p>Reasoning models think before they answer, producing a long internal chain of thought before responding to the user. </p> <h3 id=a-how-reasoning-works><strong>a. How reasoning works</strong><a class=headerlink href=#a-how-reasoning-works title="Permanent link">&para;</a></h3> <p>For example: the o1 models introduce reasoning tokens. The models use these reasoning tokens to "think", <strong>breaking down their understanding of the prompt and considering multiple approaches to generating a response.</strong> After generating reasoning tokens, the model produces an answer as visible completion tokens, and discards the reasoning tokens from its context.</p> <p>Here is an example of a multi-step conversation between a user and an assistant. Input and output tokens from each step are carried over, while reasoning tokens are discarded. <img alt src=./img/reasoning_tokens.png></p> <h3 id=b-advice-on-reasoning-model-prompting><strong>b. Advice on reasoning model prompting</strong><a class=headerlink href=#b-advice-on-reasoning-model-prompting title="Permanent link">&para;</a></h3> <p>These models perform best with straightforward prompts. Some prompt engineering techniques, like few-shot learning or instructing the model to "think step by step," may not enhance performance (and can sometimes hinder it). Here are some best practices:</p> <ul> <li><strong>Keep prompts simple and direct:</strong> The models excel at understanding and responding to brief, clear instructions without the need for extensive guidance.</li> <li><strong>Avoid chain-of-thought prompts:</strong> Since these models perform reasoning internally, prompting them to "think step by step" or "explain your reasoning" is unnecessary.</li> <li><strong>Use delimiters for clarity:</strong> Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input, helping the model interpret different sections appropriately.</li> <li><strong>Try zero shot first, then few shot if needed:</strong> Reasoning models often don't need few-shot examples to produce good results, so try to write prompts without examples first. If you have more complex requirements for your desired output, it may help to include a few examples of inputs and desired outputs in your prompt. Just ensure that the examples align very closely with your prompt instructions, as discrepancies between the two may produce poor results</li> <li><strong>Limit additional context in retrieval-augmented generation (RAG):</strong> When providing additional context or documents, include only the most relevant information to prevent the model from overcomplicating its response.</li> </ul> <h3 id=c-below-example-illustrates-the-difference-between-the-non-reasoning-model-and-the-reasoning-model>c. <strong>Below example illustrates the difference between the non-reasoning model and the reasoning model:</strong><a class=headerlink href=#c-below-example-illustrates-the-difference-between-the-non-reasoning-model-and-the-reasoning-model title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Non-reasoning model outputs the response right away</strong> <img alt src=./img/gpt-4o.png></p> </li> <li> <p><strong>Internal reasoning process occurs in reasoning model</strong> <img alt src=./img/resonning_model.png></p> </li> </ul> <p>Congratulations! You've made it through the entire prompt engineering guide. I hope you've found it helpful and informative. Happy prompting!</p> <h2 id=__comments>Comments</h2> <script src=https://giscus.app/client.js data-repo=jxnl/blog data-repo-id=R_kgDOLBXJlg data-category=General data-category-id=DIC_kwDOLBXJls4CcPmr data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async>
</script> <!-- Synchronize Giscus theme with palette --> <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../01/12/building-effective-agents/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Building effective agents"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Building effective agents </div> </div> </a> <a href=../../../04/15/why-reinforcement-learning-rl-is-hot-again/ class="md-footer__link md-footer__link--next" aria-label="Next: Why Reinforcement Learning (RL) is hot again?"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Why Reinforcement Learning (RL) is hot again? </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 Danny Zheng </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=feed_rss_created.xml target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <a href=https://github.com/AICrafterZheng target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.linkedin.com/in/hongbo-zheng-engineer/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "blog"], "search": "../../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../../javascripts/mathjax.js></script> <script src=../../../../javascripts/analytics.js></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>