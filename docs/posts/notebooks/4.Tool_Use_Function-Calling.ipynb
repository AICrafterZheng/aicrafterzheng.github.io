{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Use\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_completion, AZURE_OPENAI_API_GPT_35_MODEL, AZURE_OPENAI_API_GPT_4o_MODEL, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================LLM gpt-35-turbo response======================\n",
      "I'm sorry, but I am unable to provide real-time information such as the current time. You can easily check the current time in San Francisco by using a clock, your phone, or by searching online.\n"
     ]
    }
   ],
   "source": [
    "question = \"what's current time in San Francisco?\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "response = get_completion(question, system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "While it might seem conceptually complex at first, tool use, a.k.a. function calling, is actually quite simple! You already know all the skills necessary to implement tool use, which is really just a combination of substitution and prompt chaining.\n",
    "\n",
    "In previous substitution exercises, we substituted text into prompts. With tool use, we substitute tool or function results into prompts. LLM can't literally call or access tools and functions. Instead, we have LLM:\n",
    "1. Output the tool name and arguments it wants to call\n",
    "2. Halt any further response generation while the tool is called\n",
    "3. Then we reprompt with the appended tool results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling is useful because it expands LLM's capabilities and enables LLM to handle much more complex, multi-step tasks.\n",
    "Some examples of functions you can give LLM:\n",
    "- Calculator\n",
    "- Word counter\n",
    "- SQL database querying and data retrieval\n",
    "- Weather API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "We will demonstrate a simple toy function call that can check the time in three hardcoded locations with a single tool/function defined. We have added print statements to help make the code execution easier to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import json\n",
    "deployment_name = AZURE_OPENAI_API_GPT_4o_MODEL\n",
    "# Simplified timezone data\n",
    "TIMEZONE_DATA = {\n",
    "    \"tokyo\": \"Asia/Tokyo\",\n",
    "    \"san francisco\": \"America/Los_Angeles\",\n",
    "    \"paris\": \"Europe/Paris\"\n",
    "}\n",
    "\n",
    "def get_current_time(location):\n",
    "    \"\"\"Get the current time for a given location\"\"\"\n",
    "    print(f\"get_current_time called with location: {location}\")  \n",
    "    location_lower = location.lower()\n",
    "    \n",
    "    for key, timezone in TIMEZONE_DATA.items():\n",
    "        if key in location_lower:\n",
    "            print(f\"Timezone found for {key}\")  \n",
    "            current_time = datetime.now(ZoneInfo(timezone)).strftime(\"%I:%M %p\")\n",
    "            return json.dumps({\n",
    "                \"location\": location,\n",
    "                \"current_time\": current_time\n",
    "            })\n",
    "    \n",
    "    print(f\"No timezone data found for {location_lower}\")  \n",
    "    return json.dumps({\"location\": location, \"current_time\": \"unknown\"})\n",
    "\n",
    "def run_conversation(prompt):\n",
    "    # Initial user message\n",
    "    #messages = [{\"role\": \"user\", \"content\": \"What's the current time in San Francisco, Tokyo, and Paris?\"}] # Parallel function call with a single tool/function defined\n",
    "\n",
    "    # Define the function for the model\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}] \n",
    "    # First API call: Ask the model to use the function\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Process the model's response\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "\n",
    "    # print(\"Model's response:\")\n",
    "    # print(response_message)  \n",
    "\n",
    "    # Handle function calls\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            if tool_call.function.name == \"get_current_time\":\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Function arguments: {function_args}\")  \n",
    "                time_response = get_current_time(\n",
    "                    location=function_args.get(\"location\").lower()\n",
    "                )\n",
    "                print(f\"Function response: {time_response}\")\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"get_current_time\",\n",
    "                    \"content\": time_response,\n",
    "                })\n",
    "    else:\n",
    "        print(\"No tool calls were made by the model.\")  \n",
    "\n",
    "    # Second API call: Get the final response from the model\n",
    "    print(f\"Second API call message: {messages}\")\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    final_response = final_response.choices[0].message.content\n",
    "    print(\"=========The Answer is below=========\")\n",
    "    print(final_response)\n",
    "    return final_response\n",
    "run_conversation(\"What's the current time in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool choice\n",
    "By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.\n",
    "\n",
    "- **Auto:** (Default) Call zero, one, or multiple functions. tool_choice: \"auto\"\n",
    "- **Required:** Call one or more functions. tool_choice: \"required\"\n",
    "- **Forced Function:** Call exactly one specific function. tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel function calling with multiple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_UY1L6yCY46AKxORly4cNHizl', function=Function(arguments='{\"location\": \"San Francisco\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_SpMK3spkrqdCVb9flJh1Dpce', function=Function(arguments='{\"location\": \"San Francisco\"}', name='get_current_time'), type='function'), ChatCompletionMessageToolCall(id='call_ek1oKgwRoBcKl87Ct326sUWb', function=Function(arguments='{\"location\": \"Tokyo\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_1XZXzGYjbBmnZ2CxVn6GkZAp', function=Function(arguments='{\"location\": \"Tokyo\"}', name='get_current_time'), type='function'), ChatCompletionMessageToolCall(id='call_gE45qX1tWzYN0z46djLd9nIe', function=Function(arguments='{\"location\": \"Paris\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_kuapNLl5pww3GApOQsPhEAn2', function=Function(arguments='{\"location\": \"Paris\"}', name='get_current_time'), type='function')])\n",
      "Function call: get_current_weather\n",
      "Function arguments: {'location': 'San Francisco'}\n",
      "get_current_weather called with location: San Francisco, unit: None\n",
      "Weather data found for san francisco\n",
      "Function call: get_current_time\n",
      "Function arguments: {'location': 'San Francisco'}\n",
      "get_current_time called with location: San Francisco\n",
      "Timezone found for san francisco\n",
      "Function call: get_current_weather\n",
      "Function arguments: {'location': 'Tokyo'}\n",
      "get_current_weather called with location: Tokyo, unit: None\n",
      "Weather data found for tokyo\n",
      "Function call: get_current_time\n",
      "Function arguments: {'location': 'Tokyo'}\n",
      "get_current_time called with location: Tokyo\n",
      "Timezone found for tokyo\n",
      "Function call: get_current_weather\n",
      "Function arguments: {'location': 'Paris'}\n",
      "get_current_weather called with location: Paris, unit: None\n",
      "Weather data found for paris\n",
      "Function call: get_current_time\n",
      "Function arguments: {'location': 'Paris'}\n",
      "get_current_time called with location: Paris\n",
      "Timezone found for paris\n",
      "Here's the current information:\n",
      "\n",
      "- **San Francisco**: 72°F and the time is 03:13 PM.\n",
      "- **Tokyo**: 10°C and the time is 08:13 AM.\n",
      "- **Paris**: 22°C and the time is 12:13 AM.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Provide the model deployment name you want to use for this example\n",
    "\n",
    "deployment_name = AZURE_OPENAI_API_GPT_4o_MODEL\n",
    "\n",
    "# Simplified weather data\n",
    "WEATHER_DATA = {\n",
    "    \"tokyo\": {\"temperature\": \"10\", \"unit\": \"celsius\"},\n",
    "    \"san francisco\": {\"temperature\": \"72\", \"unit\": \"fahrenheit\"},\n",
    "    \"paris\": {\"temperature\": \"22\", \"unit\": \"celsius\"}\n",
    "}\n",
    "\n",
    "# Simplified timezone data\n",
    "TIMEZONE_DATA = {\n",
    "    \"tokyo\": \"Asia/Tokyo\",\n",
    "    \"san francisco\": \"America/Los_Angeles\",\n",
    "    \"paris\": \"Europe/Paris\"\n",
    "}\n",
    "\n",
    "def get_current_weather(location, unit=None):\n",
    "    \"\"\"Get the current weather for a given location\"\"\"\n",
    "    print(f\"get_current_weather called with location: {location}, unit: {unit}\")  \n",
    "    location_lower = location.lower()\n",
    "\n",
    "    for key in WEATHER_DATA:\n",
    "        if key in location_lower:\n",
    "            print(f\"Weather data found for {key}\")  \n",
    "            weather = WEATHER_DATA[key]\n",
    "            return json.dumps({\n",
    "                \"location\": location,\n",
    "                \"temperature\": weather[\"temperature\"],\n",
    "                \"unit\": unit if unit else weather[\"unit\"]\n",
    "            })\n",
    "    \n",
    "    print(f\"No weather data found for {location_lower}\")  \n",
    "    return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "def get_current_time(location):\n",
    "    \"\"\"Get the current time for a given location\"\"\"\n",
    "    print(f\"get_current_time called with location: {location}\")  \n",
    "    location_lower = location.lower()\n",
    "    \n",
    "    for key, timezone in TIMEZONE_DATA.items():\n",
    "        if key in location_lower:\n",
    "            print(f\"Timezone found for {key}\")  \n",
    "            current_time = datetime.now(ZoneInfo(timezone)).strftime(\"%I:%M %p\")\n",
    "            return json.dumps({\n",
    "                \"location\": location,\n",
    "                \"current_time\": current_time\n",
    "            })\n",
    "    \n",
    "    print(f\"No timezone data found for {location_lower}\")  \n",
    "    return json.dumps({\"location\": location, \"current_time\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Initial user message\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather and current time in San Francisco, Tokyo, and Paris?\"}]\n",
    "\n",
    "    # Define the functions for the model\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First API call: Ask the model to use the functions\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Process the model's response\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "\n",
    "    print(\"Model's response:\")  \n",
    "    print(response_message)  \n",
    "\n",
    "    # Handle function calls\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            print(f\"Function call: {function_name}\")  \n",
    "            print(f\"Function arguments: {function_args}\")  \n",
    "            \n",
    "            if function_name == \"get_current_weather\":\n",
    "                function_response = get_current_weather(\n",
    "                    location=function_args.get(\"location\"),\n",
    "                    unit=function_args.get(\"unit\")\n",
    "                )\n",
    "            elif function_name == \"get_current_time\":\n",
    "                function_response = get_current_time(\n",
    "                    location=function_args.get(\"location\")\n",
    "                )\n",
    "            else:\n",
    "                function_response = json.dumps({\"error\": \"Unknown function\"})\n",
    "            \n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            })\n",
    "    else:\n",
    "        print(\"No tool calls were made by the model.\")  \n",
    "\n",
    "    # Second API call: Get the final response from the model\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Run the conversation and print the result\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering with functions \n",
    "When you define a function as part of your request, the details are injected into the system message using specific syntax that the model has been trained on. This means that functions consume tokens in your prompt and that you can apply prompt engineering techniques to optimize the performance of your function calls. The model uses the full context of the prompt to determine if a function should be called including function definition, the system message, and the user messages.\n",
    "\n",
    "### Improving quality and reliability\n",
    "If the model isn't calling your function when or how you expect, there are a few things you can try to improve the quality.\n",
    "\n",
    "### 1. Provide more details in your function definition\n",
    "It's important that you provide a meaningful description of the function and provide descriptions for any parameter that might not be obvious to the model. For example, in the description for the location parameter, you could include extra details and examples on the format of the location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"location\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": \"The location of the hotel. The location should include the city and the state's abbreviation (i.e. Seattle, WA or Miami, FL)\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Provide more context in the system message\n",
    "The system message can also be used to provide more context to the model. For example, if you have a function called search_hotels you could include a system message like the following to instruct the model to call the function when a user asks for help with finding a hotel.\n",
    "\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"You're an AI assistant designed to help users search for hotels. When a user asks for help finding a hotel, you should call the search_hotels function.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instruct the model to ask clarifying questions\n",
    "In some cases, you want to instruct the model to ask clarifying questions to prevent making assumptions about what values to use with functions. For example, with search_hotels you would want the model to ask for clarification if the user request didn't include details on location. To instruct the model to ask a clarifying question, you could include content like the next example in your system message.\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"Don't make assumptions about what values to use with functions. Ask for clarification if a user request is ambiguous.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Offload the burden from the model and use code where possible.\n",
    "\n",
    "- Don't make the model fill arguments you already know. \n",
    "- Combine functions that are always called in sequence. For example, if you always call `mark_location()` after `query_location()`, just move the marking logic into the query function call.\n",
    "\n",
    "### 5. Keep the number of functions small for higher accuracy.\n",
    "\n",
    "- **Evaluate your performance** with different numbers of functions.\n",
    "- **Aim for fewer than 20 functions** at any one time, though this is just a soft suggestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "You've learned how to use function calling, you're ready to move to the next chapter. Happy prompting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
